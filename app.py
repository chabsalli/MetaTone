# app.py
import os
import re
import json
import uuid
import sqlite3
from datetime import datetime, date, timedelta
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

# Charts
import plotly.graph_objects as go

# Local similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ----------------------------
# OpenAI client (requires openai>=1.0.0)
# ----------------------------
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


# ============================
# App Config
# ============================
APP_TITLE = "MetaTone â€” ì„±ì¥ ê¸°ë¡ ê¸°ë°˜ ì†Œí”„íŠ¸ìŠ¤í‚¬ íŠ¸ë˜ì»¤ (MVP)"
DB_PATH = "metatone.db"

DEFAULT_MODEL = "gpt-4o-mini"
MODEL_OPTIONS = [DEFAULT_MODEL, "gpt-4.1-mini", "gpt-4o"]

SOFT_SKILLS = ["ë¬¸ì œí•´ê²°", "ì˜ì‚¬ì†Œí†µ", "í˜‘ì—…", "ë¦¬ë”ì‹­", "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±", "í•™ìŠµì—­ëŸ‰"]
CATEGORIES = ["í•™ìŠµ(ìˆ˜ì—…/ìê²©ì¦/ë…ì„œ)", "í”„ë¡œì íŠ¸", "ë¦¬ë”ì‹­Â·ë™ì•„ë¦¬", "ëŒ€ì™¸í™œë™", "ê´€ê³„Â·í˜‘ì—…", "ìƒí™œÂ·ë£¨í‹´"]
ANALYSIS_ENGINES = ["ë¬´ë£Œ(ë¡œì»¬) â€” ê·œì¹™/TF-IDF", "LLM(OpenAI)"]
DEFAULT_ENGINE = ANALYSIS_ENGINES[0]

SKILL_CONCEPTS = {
    "ë¬¸ì œí•´ê²°": "ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ì›ì¸ì„ íŒŒì•…í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŒ€ì•ˆì„ ë§Œë“¤ê³  ê²€ì¦í•˜ëŠ” ì—­ëŸ‰",
    "ì˜ì‚¬ì†Œí†µ": "ìƒëŒ€ì˜ ì´í•´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì¡°í™”Â·ì „ë‹¬í•˜ê³  í•©ì˜ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì—­ëŸ‰",
    "í˜‘ì—…": "ì—­í• Â·ì˜ì¡´ì„±ì„ ë§ì¶”ê³  ìƒí˜¸ ì‹ ë¢°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„±ê³¼ë¥¼ í•¨ê»˜ ë§Œë“œëŠ” ì—­ëŸ‰",
    "ë¦¬ë”ì‹­": "ë°©í–¥ì„ ì œì‹œí•˜ê³  ì˜ì‚¬ê²°ì •ì„ ë•ê³  êµ¬ì„±ì›ì´ ì›€ì§ì´ê²Œ ë§Œë“œëŠ” ì˜í–¥ë ¥",
    "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": "ì—ë„ˆì§€Â·ê°ì •Â·ì‹œê°„ì„ ê´€ë¦¬í•˜ë©° ì••ë°• ì†ì—ì„œë„ íšŒë³µí•˜ê³  ì§€ì†í•˜ëŠ” ì—­ëŸ‰",
    "í•™ìŠµì—­ëŸ‰": "í•™ìŠµ ëª©í‘œë¥¼ ì„¸ìš°ê³  í”¼ë“œë°±ì„ í†µí•´ ì§€ì‹ì„ ë‚´ ê²ƒìœ¼ë¡œ ë§Œë“œëŠ” ì—­ëŸ‰",
}

# 2+2 ê¸°ë³¸
PRACTICE_N = 2
QUESTION_N = 2
ALT_ACTION_N = 2  # "ëŒ€ì•ˆí–‰ë™ 2ê°œ"ë¥¼ ì„±ì¥í”Œëœì— í¬í•¨(LLM/ë¡œì»¬ ë‘˜ ë‹¤)


# ============================
# DB Utilities
# ============================
def get_conn() -> sqlite3.Connection:
    return sqlite3.connect(DB_PATH, check_same_thread=False, timeout=10)


def init_db() -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")

        # entries: ê¸°ì¡´ê³¼ ìµœëŒ€í•œ í˜¸í™˜ (title/tagsëŠ” ë¯¸ì‚¬ìš©ì´ì–´ë„ ìœ ì§€)
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS entries (
                id TEXT PRIMARY KEY,
                created_at TEXT NOT NULL,
                entry_date TEXT NOT NULL,
                category TEXT,
                tags TEXT,
                title TEXT,
                raw_text TEXT NOT NULL,
                artifacts TEXT,
                analysis_json TEXT
            )
            """
        )

        # structured inputs: í–‰ë™/ê°ì •/ê²°ê³¼ ë¶„ë¦¬ ì €ì¥
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS entry_structured (
                entry_id TEXT PRIMARY KEY,
                actions_text TEXT,
                emotions_text TEXT,
                results_text TEXT,
                updated_at TEXT NOT NULL
            )
            """
        )

        # notes: entry_id + skill_name ë‹¨ìœ„ë¡œ practice/question ê°ê° 0..1 ì €ì¥
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS skill_notes (
                id TEXT PRIMARY KEY,
                entry_id TEXT NOT NULL,
                entry_date TEXT NOT NULL,
                skill_name TEXT NOT NULL,
                note_type TEXT NOT NULL, -- 'practice' | 'question' | 'alt_action'
                item_index INTEGER NOT NULL, -- 0..N
                item_text TEXT NOT NULL,
                memo_text TEXT,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                UNIQUE(entry_id, skill_name, note_type, item_index)
            )
            """
        )

        # ì²´í¬ë°•ìŠ¤(ì‹¤í–‰ ì—¬ë¶€) ì €ì¥
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS checklist (
                id TEXT PRIMARY KEY,
                entry_id TEXT NOT NULL,
                entry_date TEXT NOT NULL,
                skill_name TEXT NOT NULL,
                item_type TEXT NOT NULL, -- 'practice' | 'alt_action'
                item_index INTEGER NOT NULL,
                item_text TEXT NOT NULL,
                is_done INTEGER NOT NULL DEFAULT 0,
                updated_at TEXT NOT NULL,
                UNIQUE(entry_id, skill_name, item_type, item_index)
            )
            """
        )

        conn.commit()


def safe_json_loads(x: Any, default: Any) -> Any:
    if not x:
        return default
    try:
        return json.loads(x)
    except Exception:
        return default


def insert_entry(entry: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            INSERT INTO entries
            (id, created_at, entry_date, category, tags, title, raw_text, artifacts, analysis_json)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                entry["id"],
                entry["created_at"],
                entry["entry_date"],
                entry.get("category"),
                json.dumps(entry.get("tags", []), ensure_ascii=False),
                entry.get("title"),
                entry["raw_text"],
                json.dumps(entry.get("artifacts", []), ensure_ascii=False),
                json.dumps(entry.get("analysis", {}), ensure_ascii=False),
            ),
        )
        conn.commit()


def upsert_structured(entry_id: str, actions_text: str, emotions_text: str, results_text: str) -> None:
    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            INSERT INTO entry_structured (entry_id, actions_text, emotions_text, results_text, updated_at)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(entry_id) DO UPDATE SET
                actions_text = excluded.actions_text,
                emotions_text = excluded.emotions_text,
                results_text = excluded.results_text,
                updated_at = excluded.updated_at
            """,
            (entry_id, actions_text or "", emotions_text or "", results_text or "", now),
        )
        conn.commit()


def fetch_structured(entry_id: str) -> Dict[str, str]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            "SELECT actions_text, emotions_text, results_text FROM entry_structured WHERE entry_id = ?",
            (entry_id,),
        )
        row = cur.fetchone()
    if not row:
        return {"actions_text": "", "emotions_text": "", "results_text": ""}
    return {"actions_text": row[0] or "", "emotions_text": row[1] or "", "results_text": row[2] or ""}


def update_entry_analysis(entry_id: str, analysis: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            "UPDATE entries SET analysis_json = ? WHERE id = ?",
            (json.dumps(analysis, ensure_ascii=False), entry_id),
        )
        conn.commit()


def delete_entry(entry_id: str) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("DELETE FROM entries WHERE id = ?", (entry_id,))
        cur.execute("DELETE FROM skill_notes WHERE entry_id = ?", (entry_id,))
        cur.execute("DELETE FROM entry_structured WHERE entry_id = ?", (entry_id,))
        cur.execute("DELETE FROM checklist WHERE entry_id = ?", (entry_id,))
        conn.commit()


def fetch_entries(limit: int = 1000) -> pd.DataFrame:
    with get_conn() as conn:
        df = pd.read_sql_query(
            "SELECT * FROM entries ORDER BY entry_date DESC, created_at DESC LIMIT ?",
            conn,
            params=(limit,),
        )

    if df.empty:
        df["tags_parsed"] = []
        df["artifacts_parsed"] = []
        df["analysis_parsed"] = []
        return df

    df["tags_parsed"] = df["tags"].apply(lambda x: safe_json_loads(x, default=[]))
    df["artifacts_parsed"] = df["artifacts"].apply(lambda x: safe_json_loads(x, default=[]))
    df["analysis_parsed"] = df["analysis_json"].apply(lambda x: safe_json_loads(x, default={}))
    return df


def fetch_entry_by_id(entry_id: str) -> Optional[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("SELECT * FROM entries WHERE id = ?", (entry_id,))
        row = cur.fetchone()

    if not row:
        return None

    cols = ["id", "created_at", "entry_date", "category", "tags", "title", "raw_text", "artifacts", "analysis_json"]
    d = dict(zip(cols, row))

    d["tags"] = safe_json_loads(d.get("tags"), default=[])
    d["artifacts"] = safe_json_loads(d.get("artifacts"), default=[])
    d["analysis_json"] = safe_json_loads(d.get("analysis_json"), default={})
    return d


# ============================
# Notes / Checklist
# ============================
def upsert_skill_note(
    entry_id: str,
    entry_date: str,
    skill_name: str,
    note_type: str,  # practice|question|alt_action
    item_index: int,
    item_text: str,
    memo_text: str,
) -> None:
    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            INSERT INTO skill_notes
            (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(entry_id, skill_name, note_type, item_index) DO UPDATE SET
                item_text = excluded.item_text,
                memo_text = excluded.memo_text,
                updated_at = excluded.updated_at
            """,
            (
                str(uuid.uuid4()),
                entry_id,
                entry_date,
                skill_name,
                note_type,
                int(item_index),
                item_text or "",
                memo_text or "",
                now,
                now,
            ),
        )
        conn.commit()


def fetch_skill_notes_for_entry(entry_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE entry_id = ?
            ORDER BY skill_name, note_type, item_index
            """,
            (entry_id,),
        )
        rows = cur.fetchall()

    out: List[Dict[str, Any]] = []
    for r in rows:
        out.append(
            {
                "entry_id": r[0],
                "entry_date": r[1],
                "skill_name": r[2],
                "note_type": r[3],
                "item_index": int(r[4]),
                "item_text": r[5] or "",
                "memo_text": r[6] or "",
                "updated_at": r[7] or "",
            }
        )
    return out


def fetch_skill_notes_by_skill(skill_name: str, limit: int = 500) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE skill_name = ?
            ORDER BY entry_date DESC, updated_at DESC
            LIMIT ?
            """,
            (skill_name, limit),
        )
        rows = cur.fetchall()

    out: List[Dict[str, Any]] = []
    for r in rows:
        out.append(
            {
                "entry_id": r[0],
                "entry_date": r[1],
                "skill_name": r[2],
                "note_type": r[3],
                "item_index": int(r[4]),
                "item_text": r[5] or "",
                "memo_text": r[6] or "",
                "updated_at": r[7] or "",
            }
        )
    return out


def fetch_skill_notes_by_date(entry_date: str, limit: int = 500) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE entry_date = ?
            ORDER BY skill_name, note_type, item_index
            LIMIT ?
            """,
            (entry_date, limit),
        )
        rows = cur.fetchall()

    out: List[Dict[str, Any]] = []
    for r in rows:
        out.append(
            {
                "entry_id": r[0],
                "entry_date": r[1],
                "skill_name": r[2],
                "note_type": r[3],
                "item_index": int(r[4]),
                "item_text": r[5] or "",
                "memo_text": r[6] or "",
                "updated_at": r[7] or "",
            }
        )
    return out


def upsert_checklist(
    entry_id: str,
    entry_date: str,
    skill_name: str,
    item_type: str,  # practice|alt_action
    item_index: int,
    item_text: str,
    is_done: bool,
) -> None:
    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            INSERT INTO checklist
            (id, entry_id, entry_date, skill_name, item_type, item_index, item_text, is_done, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(entry_id, skill_name, item_type, item_index) DO UPDATE SET
                item_text = excluded.item_text,
                is_done = excluded.is_done,
                updated_at = excluded.updated_at
            """,
            (
                str(uuid.uuid4()),
                entry_id,
                entry_date,
                skill_name,
                item_type,
                int(item_index),
                item_text or "",
                1 if is_done else 0,
                now,
            ),
        )
        conn.commit()


def fetch_checklist_for_entry(entry_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            """
            SELECT entry_id, entry_date, skill_name, item_type, item_index, item_text, is_done, updated_at
            FROM checklist
            WHERE entry_id = ?
            ORDER BY skill_name, item_type, item_index
            """,
            (entry_id,),
        )
        rows = cur.fetchall()

    out: List[Dict[str, Any]] = []
    for r in rows:
        out.append(
            {
                "entry_id": r[0],
                "entry_date": r[1],
                "skill_name": r[2],
                "item_type": r[3],
                "item_index": int(r[4]),
                "item_text": r[5] or "",
                "is_done": bool(r[6]),
                "updated_at": r[7] or "",
            }
        )
    return out


def group_notes(notes: List[Dict[str, Any]]) -> Dict[Tuple[str, str], Dict[int, Dict[str, str]]]:
    out: Dict[Tuple[str, str], Dict[int, Dict[str, str]]] = {}
    for n in notes:
        k = (n["skill_name"], n["note_type"])
        out.setdefault(k, {})
        out[k][int(n["item_index"])] = {"item_text": n.get("item_text", ""), "memo_text": n.get("memo_text", "")}
    return out


def group_checklist(items: List[Dict[str, Any]]) -> Dict[Tuple[str, str], Dict[int, Dict[str, Any]]]:
    out: Dict[Tuple[str, str], Dict[int, Dict[str, Any]]] = {}
    for it in items:
        k = (it["skill_name"], it["item_type"])
        out.setdefault(k, {})
        out[k][int(it["item_index"])] = it
    return out


# ============================
# Similarity (local) + caching
# ============================
@st.cache_resource(show_spinner=False)
def build_similarity_index_cached(corpus: Tuple[str, ...]) -> Tuple[TfidfVectorizer, Any]:
    vectorizer = TfidfVectorizer(stop_words=None, max_features=5000)
    X = vectorizer.fit_transform(list(corpus))
    return vectorizer, X


def get_similar_entries(df: pd.DataFrame, target_text: str, top_k: int = 5) -> List[Tuple[str, float]]:
    if top_k <= 0 or df.empty:
        return []
    corpus_list = df["raw_text"].fillna("").tolist()
    if len(corpus_list) < 2:
        return []

    corpus = tuple(corpus_list)
    vectorizer, X = build_similarity_index_cached(corpus)
    try:
        x_target = vectorizer.transform([target_text])
        sims = cosine_similarity(x_target, X).flatten()
    except Exception:
        return []

    pairs = list(zip(df["id"].tolist(), sims.tolist()))
    pairs.sort(key=lambda x: x[1], reverse=True)
    return pairs[:top_k]


# ============================
# Robust JSON parsing (LLM)
# ============================
def strip_code_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^\s*```(?:json)?\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\s*```\s*$", "", s)
    return s.strip()


def _extract_first_json_object(s: str) -> str:
    s = strip_code_fences(s)
    start = s.find("{")
    if start == -1:
        return s

    depth = 0
    for i in range(start, len(s)):
        if s[i] == "{":
            depth += 1
        elif s[i] == "}":
            depth -= 1
        if depth == 0:
            return s[start : i + 1]
    return s


def _json_repair_minimal(s: str) -> str:
    s = (s or "").strip()
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = re.sub(r",\s*([}\]])", r"\1", s)  # trailing comma
    s = re.sub(r"\bTrue\b", "true", s)
    s = re.sub(r"\bFalse\b", "false", s)
    s = re.sub(r"\bNone\b", "null", s)
    return s


def robust_json_loads(s: str) -> Dict[str, Any]:
    raw = _extract_first_json_object(s)
    try:
        out = json.loads(raw)
    except Exception:
        out = json.loads(_json_repair_minimal(raw))
    if not isinstance(out, dict):
        raise ValueError("JSON ìµœìƒìœ„ê°€ ê°ì²´(dict)ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    return out


# ============================
# Analysis engines
# - ìš”ì•½/STAR ì—†ìŒ
# - í–‰ë™/ë°°ì›€
# - ìŠ¤í‚¬ 1~3ê°œ
# - ì„±ì¥í”Œëœ: top ìŠ¤í‚¬ ê¸°ì¤€ practices(2) + questions(2) + alt_actions(2)
# ============================
def get_openai_client(api_key: str):
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ë²„ì „ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. `pip install -U openai` í•´ì£¼ì„¸ìš”.")
    if not api_key or not api_key.strip():
        raise RuntimeError("OpenAI API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return OpenAI(api_key=api_key)


def analyze_entry_with_openai(
    api_key: str,
    model: str,
    entry: Dict[str, Any],
    related_summaries: List[Dict[str, Any]],
) -> Dict[str, Any]:
    client = get_openai_client(api_key)

    persona = (
        "ë‹¹ì‹ ì€ MetaToneì˜ ì½”ì¹˜ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ê¸°ë¡ì—ì„œ 'í–‰ë™'ê³¼ 'ë°°ì›€'ì„ ë½‘ê³ , ê·¸ ê¸°ë¡ì—ì„œ ë“œëŸ¬ë‚œ ì†Œí”„íŠ¸ìŠ¤í‚¬(1~3ê°œ)ì„ ê·¼ê±° ì¸ìš©ê³¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤. "
        "ê³¼ì¥/ë¯¸ì‚¬ì—¬êµ¬/ë‹¨ì • ê¸ˆì§€. ê·¼ê±° ì¤‘ì‹¬."
    )

    related_block: List[Dict[str, Any]] = []
    for rs in (related_summaries or [])[:5]:
        related_block.append(
            {
                "id": rs.get("id"),
                "date": rs.get("entry_date"),
                "one_liner": rs.get("one_liner", ""),
                "skills": rs.get("skills", []),
            }
        )

    output_contract: Dict[str, Any] = {
        "meta": {"entry_id": entry["id"], "entry_date": entry["entry_date"], "category": entry.get("category") or ""},
        "situation_analysis": {
            "actions": ["ë‚´ê°€ ì‹¤ì œë¡œ í•œ í–‰ë™ 2~4ê°œ(ì§§ì€ ë¬¸ì¥)"],
            "learnings": ["ë°°ì›€ 1~2ê°œ(ì§§ì€ ë¬¸ì¥)"],
        },
        "soft_skills": [
            {
                "name": "í˜‘ì—…",
                "confidence": 0.0,
                "evidence_quotes": ["ì›ë¬¸ ê·¸ëŒ€ë¡œ 1~2ê°œ(ê° 80ì ì´ë‚´)"],
                "why_it_counts": "ì™œ ì´ ì—­ëŸ‰ì¸ì§€ 1ë¬¸ì¥",
                "concept": "ê°œë… 1ë¬¸ì¥",
            }
        ],
        "growth_plan": {
            "top_skill": "í˜‘ì—…",
            "practices": ["ì—°ìŠµ/ë£¨í‹´ 1", "ì—°ìŠµ/ë£¨í‹´ 2"],
            "questions": ["ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸ 1", "ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸ 2"],
            "alt_actions": ["ëŒ€ì•ˆí–‰ë™ 1", "ëŒ€ì•ˆí–‰ë™ 2"],
        },
    }

    user_payload = {
        "entry": {
            "entry_date": entry["entry_date"],
            "category": entry.get("category"),
            "raw_text": entry["raw_text"],
            "artifacts": entry.get("artifacts") or [],
            "structured": entry.get("structured") or {},
        },
        "related_entries_hint": related_block,
        "soft_skill_candidates": SOFT_SKILLS,
        "skill_concepts": SKILL_CONCEPTS,
        "output_contract_example": output_contract,
        "constraints": {"practice_n": PRACTICE_N, "question_n": QUESTION_N, "alt_action_n": ALT_ACTION_N},
    }

    instructions = (
        "ê·œì¹™:\n"
        "1) ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥(ë§ˆí¬ë‹¤ìš´/ì½”ë“œíœìŠ¤/ì„¤ëª…ë¬¸ ê¸ˆì§€)\n"
        "2) soft_skillsëŠ” 1~3ê°œ, confidenceëŠ” 0~1 ìˆ«ì\n"
        "3) evidence_quotesëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìµœëŒ€ 2ê°œ, ê° 80ì ì´ë‚´\n"
        "4) situation_analysisëŠ” actions/learningsë§Œ (ìš”ì•½ ê¸ˆì§€)\n"
        f"5) growth_plan.practicesëŠ” ì •í™•íˆ {PRACTICE_N}ê°œ, questionsëŠ” ì •í™•íˆ {QUESTION_N}ê°œ, alt_actionsëŠ” ì •í™•íˆ {ALT_ACTION_N}ê°œ\n"
        "6) growth_plan.top_skillì€ soft_skills ì¤‘ confidenceê°€ ê°€ì¥ ë†’ì€ ìŠ¤í‚¬ëª…\n"
        "7) conceptëŠ” skill_conceptsë¥¼ ì°¸ê³ í•´ 1ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ\n"
        "8) ê³¼ì¥/ë¯¸ì‚¬ì—¬êµ¬/ë‹¨ì • ê¸ˆì§€\n"
        "9) alt_actionsëŠ” 'ë‹¹ì‹œ ê°ˆë“±/ì–´ë ¤ì›€ ìƒí™©ì—ì„œ ë‹¤ë¥¸ ì„ íƒì„ í–ˆë‹¤ë©´?' ê´€ì ìœ¼ë¡œ, êµ¬ì²´ í–‰ë™ 2ê°œë¥¼ ì œì‹œ\n"
    )

    try:
        resp = client.chat.completions.create(
            model=model,
            temperature=0.4,
            messages=[
                {"role": "system", "content": persona},
                {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                {"role": "user", "content": instructions},
            ],
        )
    except Exception as e:
        raise RuntimeError(
            f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}\n\n"
            f"ì ê²€:\n- API Key ìœ íš¨ ì—¬ë¶€\n- ëª¨ë¸({model}) ì ‘ê·¼ ê¶Œí•œ/ì´ë¦„\n- ì‚¬ìš©ëŸ‰/ì¿¼í„°/ê²°ì œ ìƒíƒœ"
        )

    out = robust_json_loads(resp.choices[0].message.content or "")
    return out


def analyze_entry_local(entry: Dict[str, Any], related_summaries: List[Dict[str, Any]]) -> Dict[str, Any]:
    text = (entry.get("raw_text") or "").strip()

    # í–‰ë™/ë°°ì›€: ë³´ìˆ˜ì  ì¶”ì¶œ
    lines = [l.strip() for l in re.split(r"[\n\r]+", text) if l.strip()]
    sentences = [s.strip() for s in re.split(r"[.!?\n]", text) if s.strip()]

    action_markers = ["í–ˆë‹¤", "í•¨", "ì§„í–‰", "ì •ë¦¬", "ê³µìœ ", "ì„¤ëª…", "ì¡°ìœ¨", "í™•ì¸", "ê°œì„ ", "ì‹œë„", "ê²°ì •", "ë¶„ì„", "ì œì•ˆ", "ìš”ì²­"]
    actions: List[str] = []
    for l in lines:
        if any(m in l for m in action_markers):
            actions.append(l[:140])
        if len(actions) >= 4:
            break
    if not actions:
        actions = [sentences[0][:140]] if sentences else ["(í–‰ë™) ë‚´ê°€ ì‹¤ì œë¡œ í•œ ì¼ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì ì–´ë³´ì„¸ìš”."]

    learning_markers = ["ë°°ì› ", "ê¹¨ë‹¬", "ë‹¤ìŒ", "ê°œì„ ", "ë°˜ì„±", "ëŠê¼ˆ", "ì•Œê²Œ", "êµí›ˆ", "ì„±ì°°"]
    learnings: List[str] = []
    for l in reversed(lines):
        if any(m in l for m in learning_markers):
            learnings.append(l[:140])
        if len(learnings) >= 2:
            break
    learnings = list(reversed(learnings))
    if not learnings:
        learnings = ["(ë°°ì›€) ì˜¤ëŠ˜ ì–»ì€ êµí›ˆ/ë‹¤ìŒ ê¸°ì¤€ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê²¨ë³´ì„¸ìš”."]

    skill_rules = {
        "ë¬¸ì œí•´ê²°": ["ë¬¸ì œ", "ì›ì¸", "í•´ê²°", "ë¶„ì„", "ë””ë²„ê¹…", "êµ¬ì¡°", "ëŒ€ì•ˆ", "ê°œì„ "],
        "ì˜ì‚¬ì†Œí†µ": ["ì„¤ëª…", "ê³µìœ ", "ë°œí‘œ", "ì„¤ë“", "ì •ë¦¬", "ë¬¸ì„œ", "í”¼ë“œë°±", "í•©ì˜"],
        "í˜‘ì—…": ["íŒ€", "í˜‘ì—…", "ì¡°ìœ¨", "ì—­í• ", "íšŒì˜", "ê°ˆë“±", "ë™ë£Œ", "í•¨ê»˜"],
        "ë¦¬ë”ì‹­": ["ì£¼ë„", "ë¦¬ë“œ", "ê²°ì •", "ë°©í–¥", "ê°€ì´ë“œ", "ì½”ì¹­", "ì±…ì„", "ê¸°íš"],
        "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": ["ì‹œê°„", "ë£¨í‹´", "íšŒë³µ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì••ë°•", "ìš°ì„ ìˆœìœ„", "ì§€ì†", "ì»¨ë””ì…˜"],
        "í•™ìŠµì—­ëŸ‰": ["ê³µë¶€", "í•™ìŠµ", "ì •ë¦¬", "ë³µìŠµ", "ì‹¤í—˜", "ê°œë…", "ê°•ì˜", "ë…ì„œ", "ì—°ìŠµ"],
    }

    scores = {k: 0 for k in SOFT_SKILLS}
    for sk, kws in skill_rules.items():
        for kw in kws:
            if kw in text:
                scores[sk] += 1

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    picked = [(k, v) for k, v in ranked if v > 0][:3]
    if not picked:
        picked = [("í•™ìŠµì—­ëŸ‰", 1)]

    evidence: Dict[str, List[str]] = {k: [] for k in SOFT_SKILLS}
    for sk, _v in picked:
        kws = skill_rules.get(sk, [])
        for s in sentences:
            if any(kw in s for kw in kws):
                evidence[sk].append(s[:80])
            if len(evidence[sk]) >= 2:
                break

    max_score = max(v for _, v in picked) if picked else 1
    soft_skills = []
    for sk, v in picked:
        conf = 0.4 + 0.6 * (v / max_score) if max_score > 0 else 0.5
        soft_skills.append(
            {
                "name": sk,
                "confidence": round(min(max(conf, 0.0), 1.0), 2),
                "evidence_quotes": evidence[sk][:2] if evidence[sk] else (sentences[:1] if sentences else []),
                "why_it_counts": "ì›ë¬¸ì—ì„œ í•´ë‹¹ í–‰ë™ ë‹¨ì„œ(í‚¤ì›Œë“œ/í‘œí˜„)ê°€ ë³´ì—¬ ì´ ì—­ëŸ‰ì´ ë“œëŸ¬ë‚œ ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. (ë¬´ë£Œ ë¡œì»¬ ë¶„ì„)",
                "concept": SKILL_CONCEPTS.get(sk, ""),
            }
        )

    top_skill = soft_skills[0]["name"]

    practices = [
        "ë‹¤ìŒ ê¸°ë¡ì—ì„œ 'ë‚´ê°€ ì„ íƒí•œ ê¸°ì¤€(ìš°ì„ ìˆœìœ„/ê·¼ê±°)'ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê¸°ê¸°",
        "ê²°ê³¼ë¥¼ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„(ì „/í›„ ë³€í™”, ì‹œê°„/íšŸìˆ˜/í’ˆì§ˆ)ë¡œ ì ê¸°",
    ][:PRACTICE_N]

    questions = [
        "ë‚´ê°€ í•œ ì„ íƒì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì´ì—ˆë‚˜?",
        "ë‹¤ìŒì— ê°™ì€ ìƒí™©ì´ë©´ ë¬´ì—‡ì„ ìœ ì§€/ë³€ê²½í• ê¹Œ?",
    ][:QUESTION_N]

    alt_actions = [
        "ê°ˆë“±/ì–´ë ¤ì›€ ìƒí™©ì—ì„œ ë¨¼ì € ìƒëŒ€ì˜ ìš”êµ¬Â·ìš°ë ¤ë¥¼ 2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ í™•ì¸í•˜ê¸°",
        "ê²°ì • ì „ì— 'ëŒ€ì•ˆ 2ê°œ + ê°ê°ì˜ ë¦¬ìŠ¤í¬ 1ê°œ'ë¥¼ ì ê³  íŒ€ê³¼ 5ë¶„ë§Œ ê³µìœ í•˜ê¸°",
    ][:ALT_ACTION_N]

    return {
        "meta": {"entry_id": entry["id"], "entry_date": entry["entry_date"], "category": entry.get("category") or ""},
        "situation_analysis": {"actions": actions[:4], "learnings": learnings[:2]},
        "soft_skills": soft_skills,
        "growth_plan": {"top_skill": top_skill, "practices": practices, "questions": questions, "alt_actions": alt_actions},
    }


def run_analysis_engine(engine: str, entry: Dict[str, Any], related: List[Dict[str, Any]]) -> Dict[str, Any]:
    if engine.startswith("ë¬´ë£Œ"):
        return analyze_entry_local(entry=entry, related_summaries=related)

    api_key = st.session_state.get("api_key", "")
    if not api_key:
        st.warning("LLM ë¶„ì„ì„ ì„ íƒí–ˆì§€ë§Œ API Keyê°€ ì—†ì–´ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return analyze_entry_local(entry=entry, related_summaries=related)

    try:
        return analyze_entry_with_openai(
            api_key=api_key,
            model=st.session_state.get("model", DEFAULT_MODEL),
            entry=entry,
            related_summaries=related,
        )
    except Exception as e:
        st.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨ â†’ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n\nì‚¬ìœ : {e}")
        return analyze_entry_local(entry=entry, related_summaries=related)


# ============================
# Aggregations / Related summaries
# ============================
def summarize_for_related(df: pd.DataFrame) -> List[Dict[str, Any]]:
    summaries: List[Dict[str, Any]] = []
    for _, r in df.iterrows():
        an = r.get("analysis_parsed") or {}
        one_liner = ""
        skills: List[str] = []
        try:
            sa = (an.get("situation_analysis", {}) or {}) if isinstance(an, dict) else {}
            learnings = sa.get("learnings") or []
            if isinstance(learnings, list) and learnings:
                one_liner = (learnings[0] or "")[:80]
            soft = (an.get("soft_skills") or []) if isinstance(an, dict) else []
            skills = [x.get("name") for x in soft if isinstance(x, dict) and x.get("name")]
        except Exception:
            pass
        summaries.append({"id": r["id"], "entry_date": r["entry_date"], "one_liner": one_liner, "skills": skills})
    return summaries


def compute_skill_vector(df: pd.DataFrame) -> Dict[str, float]:
    """
    ë°©ì‚¬í˜• ì°¨íŠ¸ìš© ì ìˆ˜(0~1):
    ê¸°ê°„ ë‚´ ìŠ¤í‚¬ confidence í‰ê· (ì—†ìœ¼ë©´ 0)
    """
    sums = {s: 0.0 for s in SOFT_SKILLS}
    cnts = {s: 0 for s in SOFT_SKILLS}
    if df.empty:
        return {s: 0.0 for s in SOFT_SKILLS}

    for an in df["analysis_parsed"].tolist():
        if not isinstance(an, dict):
            continue
        skills = an.get("soft_skills") or []
        if not isinstance(skills, list):
            continue
        for sk in skills:
            if not isinstance(sk, dict):
                continue
            name = sk.get("name")
            if name not in sums:
                continue
            try:
                conf = float(sk.get("confidence", 0.0))
            except Exception:
                conf = 0.0
            sums[name] += max(0.0, min(1.0, conf))
            cnts[name] += 1

    vec: Dict[str, float] = {}
    for s in SOFT_SKILLS:
        vec[s] = (sums[s] / cnts[s]) if cnts[s] > 0 else 0.0
    return vec


def render_radar(vec: Dict[str, float], title: str) -> None:
    labels = SOFT_SKILLS
    values = [float(vec.get(s, 0.0)) for s in labels]
    # close the loop
    labels_closed = labels + [labels[0]]
    values_closed = values + [values[0]]

    fig = go.Figure(
        data=[
            go.Scatterpolar(
                r=values_closed,
                theta=labels_closed,
                fill="toself",
            )
        ]
    )
    fig.update_layout(
        title=title,
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=False,
        margin=dict(l=30, r=30, t=60, b=30),
        height=420,
    )
    st.plotly_chart(fig, use_container_width=True)


# ============================
# UI helpers
# ============================
def get_top_skill_from_analysis(analysis: Dict[str, Any]) -> Optional[str]:
    if not isinstance(analysis, dict):
        return None
    gp = analysis.get("growth_plan", {}) or {}
    top = gp.get("top_skill")
    if top in SOFT_SKILLS:
        return top
    skills = analysis.get("soft_skills") or []
    if isinstance(skills, list) and skills and isinstance(skills[0], dict):
        name = skills[0].get("name")
        if name in SOFT_SKILLS:
            return name
    return None


def get_growth_items(analysis: Dict[str, Any]) -> Tuple[List[str], List[str], List[str]]:
    gp = (analysis.get("growth_plan", {}) or {}) if isinstance(analysis, dict) else {}
    practices = gp.get("practices") or []
    questions = gp.get("questions") or []
    alt_actions = gp.get("alt_actions") or []
    if not isinstance(practices, list):
        practices = []
    if not isinstance(questions, list):
        questions = []
    if not isinstance(alt_actions, list):
        alt_actions = []
    practices = (practices + [""] * PRACTICE_N)[:PRACTICE_N]
    questions = (questions + [""] * QUESTION_N)[:QUESTION_N]
    alt_actions = (alt_actions + [""] * ALT_ACTION_N)[:ALT_ACTION_N]
    return practices, questions, alt_actions


def ensure_notes_initialized(entry_id: str, entry_date: str, skill_name: str, practices: List[str], questions: List[str], alt_actions: List[str]) -> None:
    existing = fetch_skill_notes_for_entry(entry_id)
    exists_keys = {(n["skill_name"], n["note_type"], int(n["item_index"])) for n in existing}

    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()

        for i in range(PRACTICE_N):
            key = (skill_name, "practice", i)
            if key not in exists_keys:
                cur.execute(
                    """
                    INSERT OR IGNORE INTO skill_notes
                    (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (str(uuid.uuid4()), entry_id, entry_date, skill_name, "practice", i, practices[i] if i < len(practices) else "", "", now, now),
                )

        for i in range(QUESTION_N):
            key = (skill_name, "question", i)
            if key not in exists_keys:
                cur.execute(
                    """
                    INSERT OR IGNORE INTO skill_notes
                    (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (str(uuid.uuid4()), entry_id, entry_date, skill_name, "question", i, questions[i] if i < len(questions) else "", "", now, now),
                )

        for i in range(ALT_ACTION_N):
            key = (skill_name, "alt_action", i)
            if key not in exists_keys:
                cur.execute(
                    """
                    INSERT OR IGNORE INTO skill_notes
                    (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (str(uuid.uuid4()), entry_id, entry_date, skill_name, "alt_action", i, alt_actions[i] if i < len(alt_actions) else "", "", now, now),
                )

        conn.commit()


def ensure_checklist_initialized(entry_id: str, entry_date: str, skill_name: str, practices: List[str], alt_actions: List[str]) -> None:
    existing = fetch_checklist_for_entry(entry_id)
    exists_keys = {(x["skill_name"], x["item_type"], int(x["item_index"])) for x in existing}

    # practices
    for i in range(PRACTICE_N):
        key = (skill_name, "practice", i)
        if key not in exists_keys:
            upsert_checklist(entry_id, entry_date, skill_name, "practice", i, practices[i] if i < len(practices) else "", False)

    # alt actions
    for i in range(ALT_ACTION_N):
        key = (skill_name, "alt_action", i)
        if key not in exists_keys:
            upsert_checklist(entry_id, entry_date, skill_name, "alt_action", i, alt_actions[i] if i < len(alt_actions) else "", False)


def render_analysis_block(analysis: Dict[str, Any]) -> None:
    if not analysis or not isinstance(analysis, dict):
        st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ğŸ§  ìƒí™©ë¶„ì„")
    sa = analysis.get("situation_analysis", {}) or {}
    actions = sa.get("actions") or []
    learnings = sa.get("learnings") or []

    st.markdown("**í–‰ë™**")
    if isinstance(actions, list) and actions:
        for a in actions:
            st.write(f"- {a}")
    else:
        st.write("â€”")

    st.markdown("**ë°°ì›€**")
    if isinstance(learnings, list) and learnings:
        for l in learnings:
            st.write(f"- {l}")
    else:
        st.write("â€”")

    st.subheader("ğŸ¯ ì˜¤ëŠ˜ ë“œëŸ¬ë‚œ ì†Œí”„íŠ¸ ìŠ¤í‚¬")
    skills = analysis.get("soft_skills", []) or []
    if not isinstance(skills, list) or not skills:
        st.write("ì„ ì •ëœ ì—­ëŸ‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for sk in skills:
        if not isinstance(sk, dict):
            continue
        name = sk.get("name", "")
        try:
            conf = float(sk.get("confidence", 0))
        except Exception:
            conf = 0.0

        with st.expander(f"{name} (confidence: {conf:.2f})", expanded=False):
            ev = sk.get("evidence_quotes", []) or []
            if isinstance(ev, list) and ev:
                st.markdown("**ê·¼ê±°(ì›ë¬¸ ì¸ìš©)**")
                for q in ev[:2]:
                    st.caption(f"â€œ{q}â€")

            st.markdown("**ì™œ ì´ ìŠ¤í‚¬ì¸ê°€**")
            st.write(sk.get("why_it_counts", ""))

            st.markdown("**ê°œë… ì„¤ëª…**")
            st.info(sk.get("concept") or SKILL_CONCEPTS.get(name, ""))


def render_growth_and_memo(entry_id: str, entry_date: str, analysis: Dict[str, Any]) -> None:
    top_skill = get_top_skill_from_analysis(analysis)
    if not top_skill:
        st.info("top ìŠ¤í‚¬ì„ ê²°ì •í•  ìˆ˜ ì—†ì–´ ë©”ëª¨/ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    practices, questions, alt_actions = get_growth_items(analysis)
    ensure_notes_initialized(entry_id, entry_date, top_skill, practices, questions, alt_actions)
    ensure_checklist_initialized(entry_id, entry_date, top_skill, practices, alt_actions)

    notes = fetch_skill_notes_for_entry(entry_id)
    grouped = group_notes(notes)

    checklist_items = fetch_checklist_for_entry(entry_id)
    cgroup = group_checklist(checklist_items)

    st.subheader(f"âœ… ì‹¤í–‰ ì²´í¬ + âœï¸ ë©”ëª¨ (Top ìŠ¤í‚¬: {top_skill})")
    st.caption("ì²´í¬ë°•ìŠ¤ëŠ” 'ì‹¤ì œë¡œ í–‰ë™ìœ¼ë¡œ ì˜®ê²¼ëŠ”ì§€'ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì—ëŠ” ë‹µë³€(ë©”ëª¨)ì„ ë‚¨ê²¨ DBì— ì˜êµ¬ ì €ì¥ë©ë‹ˆë‹¤.")

    # --- Practices: checkbox + editable text + memo
    st.markdown("### 1) ì—°ìŠµ/ë£¨í‹´ (2) â€” ì²´í¬ë°•ìŠ¤")
    for i in range(PRACTICE_N):
        cur = grouped.get((top_skill, "practice"), {}).get(i, {"item_text": "", "memo_text": ""})
        ck = cgroup.get((top_skill, "practice"), {}).get(i, {"is_done": False, "item_text": cur["item_text"]})

        item_key = f"item_{entry_id}_{top_skill}_practice_{i}"
        memo_key = f"memo_{entry_id}_{top_skill}_practice_{i}"
        done_key = f"done_{entry_id}_{top_skill}_practice_{i}"

        col_a, col_b = st.columns([1, 2])
        with col_a:
            st.checkbox("ì‹¤í–‰í•¨", value=bool(ck.get("is_done", False)), key=done_key)
        with col_b:
            st.text_input(f"ì—°ìŠµ {i+1}", value=cur["item_text"], key=item_key)
            st.text_area("ë©”ëª¨", value=cur["memo_text"], key=memo_key, height=80)

    # --- Questions: user answers stored in memo_text
    st.markdown("### 2) ì„±ì°° ì§ˆë¬¸ (2) â€” ë‹µë³€ ì €ì¥")
    for i in range(QUESTION_N):
        cur = grouped.get((top_skill, "question"), {}).get(i, {"item_text": "", "memo_text": ""})
        item_key = f"item_{entry_id}_{top_skill}_question_{i}"
        memo_key = f"memo_{entry_id}_{top_skill}_question_{i}"

        st.text_input(f"ì§ˆë¬¸ {i+1}", value=cur["item_text"], key=item_key)
        st.text_area("ë‚´ ë‹µë³€", value=cur["memo_text"], key=memo_key, height=110)

    # --- Alternative actions: checkbox + memo (ë©”íƒ€ì¸ì§€ ê°•í™”)
    st.markdown("### 3) ëŒ€ì•ˆí–‰ë™ (2) â€” â€˜ê·¸ë•Œ ì´ë ‡ê²Œ í–ˆë‹¤ë©´?â€™")
    for i in range(ALT_ACTION_N):
        cur = grouped.get((top_skill, "alt_action"), {}).get(i, {"item_text": "", "memo_text": ""})
        ck = cgroup.get((top_skill, "alt_action"), {}).get(i, {"is_done": False, "item_text": cur["item_text"]})

        item_key = f"item_{entry_id}_{top_skill}_alt_{i}"
        memo_key = f"memo_{entry_id}_{top_skill}_alt_{i}"
        done_key = f"done_{entry_id}_{top_skill}_alt_{i}"

        col_a, col_b = st.columns([1, 2])
        with col_a:
            st.checkbox("ì‹¤í–‰/ì ìš©í•´ë´„", value=bool(ck.get("is_done", False)), key=done_key)
        with col_b:
            st.text_input(f"ëŒ€ì•ˆí–‰ë™ {i+1}", value=cur["item_text"], key=item_key)
            st.text_area("ë©”ëª¨(ì ìš©/ìƒìƒ ê²°ê³¼)", value=cur["memo_text"], key=memo_key, height=90)

    if st.button("ğŸ’¾ ì²´í¬/ë©”ëª¨ ì €ì¥", key=f"save_all_{entry_id}_{top_skill}"):
        # Save practices + checklist
        for i in range(PRACTICE_N):
            item_key = f"item_{entry_id}_{top_skill}_practice_{i}"
            memo_key = f"memo_{entry_id}_{top_skill}_practice_{i}"
            done_key = f"done_{entry_id}_{top_skill}_practice_{i}"

            item_text = st.session_state.get(item_key, "")
            memo_text = st.session_state.get(memo_key, "")
            is_done = bool(st.session_state.get(done_key, False))

            upsert_skill_note(entry_id, entry_date, top_skill, "practice", i, item_text, memo_text)
            upsert_checklist(entry_id, entry_date, top_skill, "practice", i, item_text, is_done)

        # Save questions (answers into memo_text)
        for i in range(QUESTION_N):
            item_key = f"item_{entry_id}_{top_skill}_question_{i}"
            memo_key = f"memo_{entry_id}_{top_skill}_question_{i}"

            item_text = st.session_state.get(item_key, "")
            memo_text = st.session_state.get(memo_key, "")
            upsert_skill_note(entry_id, entry_date, top_skill, "question", i, item_text, memo_text)

        # Save alt actions + checklist
        for i in range(ALT_ACTION_N):
            item_key = f"item_{entry_id}_{top_skill}_alt_{i}"
            memo_key = f"memo_{entry_id}_{top_skill}_alt_{i}"
            done_key = f"done_{entry_id}_{top_skill}_alt_{i}"

            item_text = st.session_state.get(item_key, "")
            memo_text = st.session_state.get(memo_key, "")
            is_done = bool(st.session_state.get(done_key, False))

            upsert_skill_note(entry_id, entry_date, top_skill, "alt_action", i, item_text, memo_text)
            upsert_checklist(entry_id, entry_date, top_skill, "alt_action", i, item_text, is_done)

        st.success("ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# ============================
# Pages
# ============================
def render_dashboard(df: pd.DataFrame) -> None:
    st.subheader("ğŸ“Š ëŒ€ì‹œë³´ë“œ")
    st.caption("ë°©ì‚¬í˜• ì°¨íŠ¸ë¡œ â€˜ê°•ì (ì ìˆ˜ê°€ ë†’ì€ ìŠ¤í‚¬)â€™ê³¼ â€˜ê¸°ë¡ì´ ë¶€ì¡±í•œ ì˜ì—­(ì ìˆ˜ê°€ ë‚®ì€ ìŠ¤í‚¬)â€™ì„ í•œëˆˆì— ë´…ë‹ˆë‹¤.")

    col1, col2 = st.columns([1, 2])
    with col1:
        mode = st.radio("ê¸°ê°„", ["ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼", "ì „ì²´"], index=0)
        if mode == "ìµœê·¼ 7ì¼":
            start = (date.today() - timedelta(days=6)).isoformat()
            dfw = df[df["entry_date"] >= start] if not df.empty else df
        elif mode == "ìµœê·¼ 30ì¼":
            start = (date.today() - timedelta(days=29)).isoformat()
            dfw = df[df["entry_date"] >= start] if not df.empty else df
        else:
            dfw = df

        st.metric("ê¸°ë¡ ìˆ˜", int(len(dfw)))

        # ê°„ë‹¨í•œ ë¶€ì¡± ì˜ì—­ Top-2
        vec = compute_skill_vector(dfw)
        low2 = sorted(vec.items(), key=lambda x: x[1])[:2]
        st.write("**ì§€ê¸ˆ ë¶€ì¡±í•´ ë³´ì´ëŠ” ì˜ì—­(Top-2)**")
        for k, v in low2:
            st.write(f"- {k}: {v:.2f}")

    with col2:
        vec = compute_skill_vector(dfw)
        render_radar(vec, title=f"ìŠ¤í‚¬ í”„ë¡œí•„(0~1) â€” {mode}")

    st.markdown("---")
    st.subheader("ğŸ—“ï¸ ì£¼ê°„ íë¦„(ìµœê·¼ 4ì£¼)")
    if df.empty:
        st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì£¼ ë‹¨ìœ„(ì›”~ì¼)ë¡œ ìŠ¤í‚¬ ì–¸ê¸‰ íšŸìˆ˜
    tmp = df.copy()
    tmp["entry_date_dt"] = pd.to_datetime(tmp["entry_date"], errors="coerce")
    tmp = tmp.dropna(subset=["entry_date_dt"])
    tmp["week_start"] = (tmp["entry_date_dt"] - pd.to_timedelta(tmp["entry_date_dt"].dt.weekday, unit="D")).dt.date

    rows: List[Dict[str, Any]] = []
    for _, r in tmp.iterrows():
        an = r.get("analysis_parsed") or {}
        if not isinstance(an, dict):
            continue
        for sk in (an.get("soft_skills") or []):
            if isinstance(sk, dict) and sk.get("name") in SOFT_SKILLS:
                rows.append({"week_start": r["week_start"], "skill": sk["name"]})

    if not rows:
        st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ë¶€ì¡±í•´ ì£¼ê°„ íë¦„ì„ í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df_rows = pd.DataFrame(rows)
    pivot = (
        df_rows.groupby(["week_start", "skill"])
        .size()
        .reset_index(name="count")
        .pivot(index="week_start", columns="skill", values="count")
        .fillna(0)
        .sort_index(ascending=False)
        .head(4)
        .sort_index()
    )
    st.dataframe(pivot, use_container_width=True)


def render_new_entry(df: pd.DataFrame) -> None:
    st.subheader("âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€")
    st.info("ì¼ê¸°ì²˜ëŸ¼ ë‚˜ì—´ ëŒ€ì‹ , **í–‰ë™ / ê°ì • / ê²°ê³¼** 3ì¹¸ì— ë‚˜ëˆ„ì–´ ì ì–´ë³´ì„¸ìš”.")

    col1, col2 = st.columns([1, 1])
    with col1:
        entry_date = st.date_input("ë‚ ì§œ", value=date.today())

        cat_choice = st.selectbox(
            "ì¹´í…Œê³ ë¦¬(ì„ íƒ)",
            options=["(ì„ íƒ ì•ˆ í•¨)"] + CATEGORIES + ["(ì§ì ‘ ì…ë ¥)"],
            index=0,
        )
        cat_custom = ""
        if cat_choice == "(ì§ì ‘ ì…ë ¥)":
            cat_custom = st.text_input("ì¹´í…Œê³ ë¦¬ ì§ì ‘ ì…ë ¥", placeholder="ì˜ˆ: ì¸í„´/í˜„ì¥ì‹¤ìŠµ, ê°œì¸ í”„ë¡œì íŠ¸, ì·¨ë¯¸ í™œë™ ë“±")

        if cat_choice == "(ì„ íƒ ì•ˆ í•¨)":
            category = None
        elif cat_choice == "(ì§ì ‘ ì…ë ¥)":
            category = cat_custom.strip() if cat_custom.strip() else None
        else:
            category = cat_choice

    with col2:
        artifacts = st.text_area(
            "ì¦ê±°/ìë£Œ ë§í¬(ì„ íƒ) â€” ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ",
            placeholder="ì˜ˆ: Notion ë§í¬, Google Doc, GitHub, ë°œí‘œìë£Œ URL ë“±",
        )
        artifacts_list = [x.strip() for x in (artifacts or "").splitlines() if x.strip()]

    st.markdown("### ğŸ§© ê¸°ë¡ ì…ë ¥(êµ¬ì¡°í™”)")
    a_col, e_col, r_col = st.columns(3)
    with a_col:
        actions_text = st.text_area("í–‰ë™", height=160, placeholder="ë‚´ê°€ ì‹¤ì œë¡œ í•œ í–‰ë™ì„ êµ¬ì²´ì ìœ¼ë¡œ")
    with e_col:
        emotions_text = st.text_area("ê°ì •", height=160, placeholder="ê·¸ë•Œ ëŠë‚€ ê°ì •/ëª¸ ìƒíƒœ/ì••ë°•ê° ë“±")
    with r_col:
        results_text = st.text_area("ê²°ê³¼", height=160, placeholder="ê²°ê³¼(ë³€í™”/í”¼ë“œë°±/ìˆ˜ì¹˜/ê´€ì°°) + ë°°ì›€")

    # raw_textëŠ” í˜¸í™˜ì„ ìœ„í•´ 3ì¹¸ì„ í•©ì³ ì €ì¥
    raw_text = "\n".join(
        [
            f"[í–‰ë™]\n{actions_text.strip()}",
            f"[ê°ì •]\n{emotions_text.strip()}",
            f"[ê²°ê³¼]\n{results_text.strip()}",
        ]
    ).strip()

    st.markdown("### ğŸ” ë¶„ì„ ì˜µì…˜")
    do_analysis = st.checkbox("ì €ì¥ í›„ ë¶„ì„ ì‹¤í–‰í•˜ê¸°", value=True)
    top_k = st.slider("ìœ ì‚¬ ê¸°ë¡ íŒíŠ¸(top-k)", min_value=0, max_value=10, value=5)

    if st.button("âœ… ì €ì¥", type="primary"):
        if not (actions_text.strip() or emotions_text.strip() or results_text.strip()):
            st.error("í–‰ë™/ê°ì •/ê²°ê³¼ ì¤‘ ìµœì†Œ í•˜ë‚˜ëŠ” ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        entry_id = str(uuid.uuid4())
        created_at = datetime.now().isoformat(timespec="seconds")

        entry = {
            "id": entry_id,
            "created_at": created_at,
            "entry_date": entry_date.isoformat(),
            "category": category,
            "tags": [],
            "title": None,
            "raw_text": raw_text,
            "artifacts": artifacts_list,
            "analysis": {},
        }
        insert_entry(entry)
        upsert_structured(entry_id, actions_text, emotions_text, results_text)

        st.success("ê¸°ë¡ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        if not do_analysis:
            return

        # related hints
        related: List[Dict[str, Any]] = []
        if top_k > 0 and not df.empty:
            sims = get_similar_entries(df, entry["raw_text"], top_k=top_k)
            hint_rows: List[Dict[str, Any]] = []
            for rid, _score in sims:
                r = fetch_entry_by_id(rid)
                if r:
                    hint_rows.append(
                        {
                            "id": r["id"],
                            "entry_date": r["entry_date"],
                            "category": r.get("category"),
                            "raw_text": r["raw_text"],
                            "artifacts": r.get("artifacts") or [],
                            "analysis_json": json.dumps(r.get("analysis_json") or {}, ensure_ascii=False),
                        }
                    )
            hint_df = pd.DataFrame(hint_rows) if hint_rows else pd.DataFrame()
            if not hint_df.empty:
                hint_df["analysis_parsed"] = hint_df["analysis_json"].apply(lambda x: safe_json_loads(x, default={}))
                related = summarize_for_related(hint_df)

        engine = st.session_state.get("engine", DEFAULT_ENGINE)

        payload = {
            "id": entry_id,
            "entry_date": entry["entry_date"],
            "category": category,
            "raw_text": raw_text,
            "artifacts": artifacts_list,
            "structured": {"actions": actions_text, "emotions": emotions_text, "results": results_text},
        }

        with st.spinner("ë¶„ì„ ì¤‘..."):
            analysis = run_analysis_engine(engine=engine, entry=payload, related=related)
            update_entry_analysis(entry_id, analysis)

        st.success("ë¶„ì„ ì™„ë£Œ!")
        render_analysis_block(analysis)
        st.markdown("---")
        render_growth_and_memo(entry_id, entry["entry_date"], analysis)


def render_history(df: pd.DataFrame) -> None:
    st.subheader("ğŸ“š ê¸°ë¡ ëª©ë¡")
    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. 'ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€'ì—ì„œ ì‘ì„±í•´ë³´ì„¸ìš”.")
        return

    colf1, colf2, colf3 = st.columns([1, 1, 2])
    with colf1:
        cat = st.selectbox("ì¹´í…Œê³ ë¦¬ í•„í„°", options=["(ì „ì²´)"] + CATEGORIES, index=0)
    with colf2:
        skill_filter = st.selectbox("ì†Œí”„íŠ¸ìŠ¤í‚¬ í•„í„°", options=["(ì „ì²´)"] + SOFT_SKILLS, index=0)
    with colf3:
        q = st.text_input("ê²€ìƒ‰(ë³¸ë¬¸)", placeholder="ì˜ˆ: ë°œí‘œ, ì¡°ìœ¨, íšŒë³µ, ê¸°ì¤€, í”¼ë“œë°±...")

    filtered = df.copy()

    if cat != "(ì „ì²´)":
        filtered = filtered[filtered["category"] == cat]

    if (q or "").strip():
        qq = q.strip().lower()
        filtered = filtered[filtered["raw_text"].str.lower().str.contains(qq, na=False)]

    if skill_filter != "(ì „ì²´)":
        def has_skill(an: Any) -> bool:
            if not isinstance(an, dict):
                return False
            skills = an.get("soft_skills", []) or []
            return any((s.get("name") == skill_filter) for s in skills if isinstance(s, dict))
        filtered = filtered[filtered["analysis_parsed"].apply(has_skill)]

    st.caption(f"ì´ {len(filtered)}ê°œ")
    engine = st.session_state.get("engine", DEFAULT_ENGINE)

    for _, r in filtered.iterrows():
        entry_id = r["id"]
        entry_date = r.get("entry_date", "")
        category = r.get("category") or "â€”"
        an = r.get("analysis_parsed") or {}
        if not isinstance(an, dict):
            an = {}

        skills = [s.get("name") for s in (an.get("soft_skills") or []) if isinstance(s, dict) and s.get("name")]
        skill_text = ", ".join(skills) if skills else "â€”"

        with st.expander(f"{entry_date} Â· ì¹´í…Œê³ ë¦¬: {category} | ìŠ¤í‚¬: {skill_text}"):
            st.write(r["raw_text"])

            artifacts = r.get("artifacts_parsed") or []
            if artifacts:
                st.markdown("**ì¦ê±°/ë§í¬**")
                for a in artifacts:
                    st.write(f"- {a}")

            # êµ¬ì¡°í™” ì…ë ¥ë„ ë³´ì—¬ì£¼ê¸°
            st.markdown("---")
            s = fetch_structured(entry_id)
            if any(v.strip() for v in s.values()):
                c1, c2, c3 = st.columns(3)
                with c1:
                    st.markdown("**í–‰ë™**")
                    st.write(s["actions_text"] or "â€”")
                with c2:
                    st.markdown("**ê°ì •**")
                    st.write(s["emotions_text"] or "â€”")
                with c3:
                    st.markdown("**ê²°ê³¼**")
                    st.write(s["results_text"] or "â€”")

            st.markdown("---")
            if an:
                render_analysis_block(an)
                st.markdown("---")
                render_growth_and_memo(entry_id, entry_date, an)
            else:
                st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ìš”.")

            colb1, colb2 = st.columns([1, 1])
            with colb1:
                if st.button("ğŸ¤– ì´ ê¸°ë¡ ë¶„ì„í•˜ê¸°", key=f"an_{entry_id}"):
                    entry = fetch_entry_by_id(entry_id)
                    if not entry:
                        st.error("ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        other = df[df["id"] != entry_id].head(80)
                        related = summarize_for_related(other) if not other.empty else []
                        structured = fetch_structured(entry_id)

                        payload = {
                            "id": entry["id"],
                            "entry_date": entry["entry_date"],
                            "category": entry.get("category"),
                            "raw_text": entry["raw_text"],
                            "artifacts": entry.get("artifacts") or [],
                            "structured": {"actions": structured["actions_text"], "emotions": structured["emotions_text"], "results": structured["results_text"]},
                        }
                        with st.spinner("ë¶„ì„ ì¤‘..."):
                            analysis = run_analysis_engine(engine=engine, entry=payload, related=related)
                            update_entry_analysis(entry_id, analysis)
                        st.success("ë¶„ì„ ì™„ë£Œ! í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                        st.rerun()

            with colb2:
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{entry_id}"):
                    delete_entry(entry_id)
                    st.success("ì‚­ì œí–ˆìŠµë‹ˆë‹¤. í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                    st.rerun()


def render_memos(df: pd.DataFrame) -> None:
    st.subheader("ğŸ“’ ë©”ëª¨/ë‹µë³€/ì²´í¬ë¦¬ìŠ¤íŠ¸")
    st.caption("Top ìŠ¤í‚¬ì˜ ì—°ìŠµ/ëŒ€ì•ˆí–‰ë™ ì²´í¬, ì„±ì°° ì§ˆë¬¸ ë‹µë³€(ë©”ëª¨)ì„ ë‚ ì§œë³„/ìŠ¤í‚¬ë³„ë¡œ í™•ì¸í•©ë‹ˆë‹¤.")

    tab1, tab2 = st.tabs(["ë‚ ì§œë³„", "ìŠ¤í‚¬ë³„"])

    with tab1:
        dates = sorted(df["entry_date"].dropna().unique().tolist(), reverse=True) if not df.empty else []
        if not dates:
            st.info("ì•„ì§ ê¸°ë¡/ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            d = st.selectbox("ë‚ ì§œ ì„ íƒ", options=dates, index=0)
            notes = fetch_skill_notes_by_date(d)
            if not notes:
                st.info("ì´ ë‚ ì§œì— ì €ì¥ëœ ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                by_entry: Dict[str, List[Dict[str, Any]]] = {}
                for n in notes:
                    by_entry.setdefault(n["entry_id"], []).append(n)

                for entry_id, ns in by_entry.items():
                    with st.expander(f"{d} Â· entry_id: {entry_id[:8]} Â· í•­ëª© {len(ns)}ê°œ"):
                        by_skill: Dict[str, List[Dict[str, Any]]] = {}
                        for n in ns:
                            by_skill.setdefault(n["skill_name"], []).append(n)

                        # ì²´í¬ë¦¬ìŠ¤íŠ¸ë„ ë¡œë“œ
                        citems = fetch_checklist_for_entry(entry_id)
                        cgroup = group_checklist(citems)

                        for sk, sk_notes in by_skill.items():
                            st.markdown(f"**{sk}**")

                            # practice / alt_action ì²´í¬ í‘œì‹œ
                            for nt in ["practice", "alt_action"]:
                                items = [x for x in sk_notes if x["note_type"] == nt]
                                if not items:
                                    continue
                                st.caption("ì—°ìŠµ/ë£¨í‹´" if nt == "practice" else "ëŒ€ì•ˆí–‰ë™")
                                for it in sorted(items, key=lambda x: int(x["item_index"])):
                                    ck = cgroup.get((sk, nt), {}).get(int(it["item_index"]), {"is_done": False})
                                    badge = "âœ…" if ck.get("is_done") else "â¬œ"
                                    st.write(f"- {badge} {it['item_text']}")
                                    if (it.get("memo_text") or "").strip():
                                        st.write(f"  â†³ ë©”ëª¨: {it['memo_text']}")

                            # question ë‹µë³€
                            qitems = [x for x in sk_notes if x["note_type"] == "question"]
                            if qitems:
                                st.caption("ì„±ì°° ì§ˆë¬¸/ë‹µë³€")
                                for it in sorted(qitems, key=lambda x: int(x["item_index"])):
                                    st.write(f"- Q: {it['item_text']}")
                                    st.write(f"  A: {(it.get('memo_text') or '').strip() or 'â€”'}")

    with tab2:
        skill = st.selectbox("ìŠ¤í‚¬ ì„ íƒ", options=SOFT_SKILLS, index=0)
        notes = fetch_skill_notes_by_skill(skill, limit=500)
        if not notes:
            st.info("ì´ ìŠ¤í‚¬ì— ì €ì¥ëœ ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        by_date: Dict[str, List[Dict[str, Any]]] = {}
        for n in notes:
            by_date.setdefault(n["entry_date"], []).append(n)

        for d in sorted(by_date.keys(), reverse=True):
            with st.expander(f"{d} Â· í•­ëª© {len(by_date[d])}ê°œ"):
                items = by_date[d]
                for nt in ["practice", "alt_action", "question"]:
                    sub = [x for x in items if x["note_type"] == nt]
                    if not sub:
                        continue
                    st.caption("ì—°ìŠµ/ë£¨í‹´" if nt == "practice" else ("ëŒ€ì•ˆí–‰ë™" if nt == "alt_action" else "ì„±ì°° ì§ˆë¬¸/ë‹µë³€"))
                    for it in sorted(sub, key=lambda x: int(x["item_index"])):
                        if nt == "question":
                            st.write(f"- Q: {it['item_text']}")
                            st.write(f"  A: {(it.get('memo_text') or '').strip() or 'â€”'}")
                        else:
                            st.write(f"- {it['item_text']}")
                            if (it.get("memo_text") or "").strip():
                                st.write(f"  â†³ ë©”ëª¨: {it['memo_text']}")
                        st.caption(f"entry_id: {it['entry_id'][:8]} Â· updated: {it['updated_at']}")


def render_debug(df: pd.DataFrame) -> None:
    st.subheader("ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸")
    st.write("í˜„ì¬ DBì— ì €ì¥ëœ ê¸°ë¡ ê°œìˆ˜:", len(df))

    st.markdown("### ìµœê·¼ 10ê°œ ê¸°ë¡ ë¯¸ë¦¬ë³´ê¸°")
    if not df.empty:
        st.dataframe(df[["entry_date", "category"]].head(10), use_container_width=True, hide_index=True)
    else:
        st.info("ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("### í™˜ê²½/ì„¤ì •")
    st.write(
        {
            "engine": st.session_state.get("engine", DEFAULT_ENGINE),
            "has_api_key": bool(st.session_state.get("api_key")),
            "model": st.session_state.get("model", DEFAULT_MODEL),
            "db_path": DB_PATH,
            "policy": "ì…ë ¥(í–‰ë™/ê°ì •/ê²°ê³¼) + ë¶„ì„(í–‰ë™/ë°°ì›€/ìŠ¤í‚¬) + ì„±ì¥(ì—°ìŠµ2/ì§ˆë¬¸2/ëŒ€ì•ˆí–‰ë™2) + ì²´í¬/ë©”ëª¨ ì €ì¥",
        }
    )

    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM skill_notes")
        note_count = cur.fetchone()[0]
        cur.execute("SELECT COUNT(*) FROM checklist")
        ck_count = cur.fetchone()[0]
        cur.execute("SELECT COUNT(*) FROM entry_structured")
        st_count = cur.fetchone()[0]

    st.write("ì €ì¥ëœ ë©”ëª¨ ê°œìˆ˜(skill_notes):", note_count)
    st.write("ì €ì¥ëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©(checklist):", ck_count)
    st.write("êµ¬ì¡°í™” ì…ë ¥ ì €ì¥(entry_structured):", st_count)


# ============================
# Main
# ============================
def main() -> None:
    st.set_page_config(page_title="MetaTone", layout="wide")
    init_db()

    # Sidebar settings
    st.sidebar.title("âš™ï¸ Settings")
    api_key_env = os.getenv("OPENAI_API_KEY", "")
    api_key_input = st.sidebar.text_input(
        "OpenAI API Key (ì„ íƒ)",
        value=st.session_state.get("api_key", api_key_env),
        type="password",
    )
    st.session_state["api_key"] = (api_key_input or "").strip()

    current_model = st.session_state.get("model", DEFAULT_MODEL)
    if current_model not in MODEL_OPTIONS:
        current_model = DEFAULT_MODEL
    st.sidebar.selectbox(
        "Model (LLM ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)",
        options=MODEL_OPTIONS,
        index=MODEL_OPTIONS.index(current_model),
        key="model",
    )

    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ§  ë¶„ì„ ì—”ì§„")
    st.sidebar.selectbox("ë¶„ì„ ë°©ì‹", options=ANALYSIS_ENGINES, index=0, key="engine")

    st.sidebar.markdown("---")
    page = st.sidebar.radio("í˜ì´ì§€", ["ğŸ“Š ëŒ€ì‹œë³´ë“œ", "âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€", "ğŸ“š ê¸°ë¡ ëª©ë¡", "ğŸ“’ ë©”ëª¨", "ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸"])

    df = fetch_entries()

    st.title(APP_TITLE)
    st.caption("ê¸°ë¡(í–‰ë™/ê°ì •/ê²°ê³¼) â†’ ë¶„ì„(í–‰ë™/ë°°ì›€/ìŠ¤í‚¬) â†’ ì„±ì¥(ì—°ìŠµ2/ì§ˆë¬¸2/ëŒ€ì•ˆí–‰ë™2) â†’ ì²´í¬ë°•ìŠ¤/ë‹µë³€/ë©”ëª¨ë¥¼ DBì— ì €ì¥")

    st.markdown("---")

    if page == "ğŸ“Š ëŒ€ì‹œë³´ë“œ":
        render_dashboard(df)
    elif page == "âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€":
        render_new_entry(df)
    elif page == "ğŸ“š ê¸°ë¡ ëª©ë¡":
        render_history(df)
    elif page == "ğŸ“’ ë©”ëª¨":
        render_memos(df)
    else:
        render_debug(df)


if __name__ == "__main__":
    main()
