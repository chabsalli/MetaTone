import os
import re
import json
import uuid
import time
import sqlite3
from datetime import datetime, date
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ----------------------------
# OpenAI client (requires openai>=1.0.0)
# ----------------------------
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


# ============================
# App Config
# ============================
APP_TITLE = "MetaTone â€” ì„±ì¥ ê¸°ë¡ ê¸°ë°˜ ì†Œí”„íŠ¸ìŠ¤í‚¬ íŠ¸ë˜ì»¤ (MVP)"
DB_PATH = "metatone.db"

DEFAULT_MODEL = "gpt-4o-mini"
MODEL_OPTIONS = [DEFAULT_MODEL, "gpt-4.1-mini", "gpt-4o"]

SOFT_SKILLS = ["ë¬¸ì œí•´ê²°", "ì˜ì‚¬ì†Œí†µ", "í˜‘ì—…", "ë¦¬ë”ì‹­", "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±", "í•™ìŠµì—­ëŸ‰"]

CATEGORIES = ["í•™ìŠµ(ìˆ˜ì—…/ìê²©ì¦/ë…ì„œ)", "í”„ë¡œì íŠ¸", "ë¦¬ë”ì‹­Â·ë™ì•„ë¦¬", "ëŒ€ì™¸í™œë™", "ê´€ê³„Â·í˜‘ì—…", "ìƒí™œÂ·ë£¨í‹´"]
ANALYSIS_ENGINES = ["ë¬´ë£Œ(ë¡œì»¬) â€” ê·œì¹™/TF-IDF", "LLM(OpenAI)"]
DEFAULT_ENGINE = ANALYSIS_ENGINES[0]

SKILL_CONCEPTS = {
    "ë¬¸ì œí•´ê²°": "ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ì›ì¸ì„ íŒŒì•…í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŒ€ì•ˆì„ ë§Œë“¤ê³  ê²€ì¦í•˜ëŠ” ì—­ëŸ‰",
    "ì˜ì‚¬ì†Œí†µ": "ìƒëŒ€ì˜ ì´í•´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì¡°í™”Â·ì „ë‹¬í•˜ê³  í•©ì˜ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì—­ëŸ‰",
    "í˜‘ì—…": "ì—­í• Â·ì˜ì¡´ì„±ì„ ë§ì¶”ê³  ìƒí˜¸ ì‹ ë¢°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„±ê³¼ë¥¼ í•¨ê»˜ ë§Œë“œëŠ” ì—­ëŸ‰",
    "ë¦¬ë”ì‹­": "ë°©í–¥ì„ ì œì‹œí•˜ê³  ì˜ì‚¬ê²°ì •ì„ ë•ê³  êµ¬ì„±ì›ì´ ì›€ì§ì´ê²Œ ë§Œë“œëŠ” ì˜í–¥ë ¥",
    "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": "ì—ë„ˆì§€Â·ê°ì •Â·ì‹œê°„ì„ ê´€ë¦¬í•˜ë©° ì••ë°• ì†ì—ì„œë„ íšŒë³µí•˜ê³  ì§€ì†í•˜ëŠ” ì—­ëŸ‰",
    "í•™ìŠµì—­ëŸ‰": "í•™ìŠµ ëª©í‘œë¥¼ ì„¸ìš°ê³  í”¼ë“œë°±ì„ í†µí•´ ì§€ì‹ì„ ë‚´ ê²ƒìœ¼ë¡œ ë§Œë“œëŠ” ì—­ëŸ‰",
}

# 2+2 ë©”ëª¨ ê¸°ë³¸ ê°œìˆ˜
PRACTICE_N = 2
QUESTION_N = 2


# ============================
# DB Utilities
# ============================
def get_conn() -> sqlite3.Connection:
    return sqlite3.connect(DB_PATH, check_same_thread=False, timeout=10)


def init_db() -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")

        # entries (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ ìœ ì§€: tags/titleì€ MetaToneì—ì„œ ë¯¸ì‚¬ìš©)
        cur.execute("""
        CREATE TABLE IF NOT EXISTS entries (
            id TEXT PRIMARY KEY,
            created_at TEXT NOT NULL,
            entry_date TEXT NOT NULL,
            category TEXT,
            tags TEXT,
            title TEXT,
            raw_text TEXT NOT NULL,
            artifacts TEXT,
            analysis_json TEXT
        )
        """)

        # notes: entry_id + skill_name ë‹¨ìœ„ë¡œ, practice/question ê°ê° 0..1 ì €ì¥
        cur.execute("""
        CREATE TABLE IF NOT EXISTS skill_notes (
            id TEXT PRIMARY KEY,
            entry_id TEXT NOT NULL,
            entry_date TEXT NOT NULL,
            skill_name TEXT NOT NULL,
            note_type TEXT NOT NULL,          -- 'practice' | 'question'
            item_index INTEGER NOT NULL,      -- 0..1
            item_text TEXT NOT NULL,
            memo_text TEXT,
            created_at TEXT NOT NULL,
            updated_at TEXT NOT NULL,
            UNIQUE(entry_id, skill_name, note_type, item_index)
        )
        """)

        conn.commit()


def insert_entry(entry: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO entries (id, created_at, entry_date, category, tags, title, raw_text, artifacts, analysis_json)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            entry["id"],
            entry["created_at"],
            entry["entry_date"],
            entry.get("category"),
            json.dumps(entry.get("tags", []), ensure_ascii=False),
            entry.get("title"),
            entry["raw_text"],
            json.dumps(entry.get("artifacts", []), ensure_ascii=False),
            json.dumps(entry.get("analysis", {}), ensure_ascii=False)
        ))
        conn.commit()


def update_entry_analysis(entry_id: str, analysis: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("UPDATE entries SET analysis_json = ? WHERE id = ?",
                    (json.dumps(analysis, ensure_ascii=False), entry_id))
        conn.commit()


def delete_entry(entry_id: str) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("DELETE FROM entries WHERE id = ?", (entry_id,))
        # notesë„ ê°™ì´ ì‚­ì œ
        cur.execute("DELETE FROM skill_notes WHERE entry_id = ?", (entry_id,))
        conn.commit()


def fetch_entries(limit: int = 500) -> pd.DataFrame:
    with get_conn() as conn:
        df = pd.read_sql_query(
            "SELECT * FROM entries ORDER BY entry_date DESC, created_at DESC LIMIT ?",
            conn,
            params=(limit,),
        )

    def safe_json(x, default):
        if not x:
            return default
        try:
            return json.loads(x)
        except Exception:
            return default

    df["tags_parsed"] = df["tags"].apply(lambda x: safe_json(x, default=[]))
    df["artifacts_parsed"] = df["artifacts"].apply(lambda x: safe_json(x, default=[]))
    df["analysis_parsed"] = df["analysis_json"].apply(lambda x: safe_json(x, default={}))
    return df


def fetch_entry_by_id(entry_id: str) -> Optional[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("SELECT * FROM entries WHERE id = ?", (entry_id,))
        row = cur.fetchone()

    if not row:
        return None

    cols = ["id", "created_at", "entry_date", "category", "tags", "title", "raw_text", "artifacts", "analysis_json"]
    d = dict(zip(cols, row))

    for k, default in [("tags", []), ("artifacts", []), ("analysis_json", {})]:
        try:
            d[k] = json.loads(d[k]) if d[k] else default
        except Exception:
            d[k] = default
    return d


def upsert_skill_note(
    entry_id: str,
    entry_date: str,
    skill_name: str,
    note_type: str,
    item_index: int,
    item_text: str,
    memo_text: str
) -> None:
    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO skill_notes (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(entry_id, skill_name, note_type, item_index)
            DO UPDATE SET
                item_text = excluded.item_text,
                memo_text = excluded.memo_text,
                updated_at = excluded.updated_at
        """, (
            str(uuid.uuid4()),
            entry_id,
            entry_date,
            skill_name,
            note_type,
            int(item_index),
            item_text or "",
            memo_text or "",
            now,
            now,
        ))
        conn.commit()


def fetch_skill_notes_for_entry(entry_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE entry_id = ?
            ORDER BY skill_name, note_type, item_index
        """, (entry_id,))
        rows = cur.fetchall()

    out = []
    for r in rows:
        out.append({
            "entry_id": r[0],
            "entry_date": r[1],
            "skill_name": r[2],
            "note_type": r[3],
            "item_index": r[4],
            "item_text": r[5],
            "memo_text": r[6],
            "updated_at": r[7],
        })
    return out


def fetch_skill_notes_by_skill(skill_name: str, limit: int = 300) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE skill_name = ?
            ORDER BY entry_date DESC, updated_at DESC
            LIMIT ?
        """, (skill_name, limit))
        rows = cur.fetchall()

    out = []
    for r in rows:
        out.append({
            "entry_id": r[0],
            "entry_date": r[1],
            "skill_name": r[2],
            "note_type": r[3],
            "item_index": r[4],
            "item_text": r[5],
            "memo_text": r[6],
            "updated_at": r[7],
        })
    return out


def fetch_skill_notes_by_date(entry_date: str, limit: int = 300) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, updated_at
            FROM skill_notes
            WHERE entry_date = ?
            ORDER BY skill_name, note_type, item_index
            LIMIT ?
        """, (entry_date, limit))
        rows = cur.fetchall()

    out = []
    for r in rows:
        out.append({
            "entry_id": r[0],
            "entry_date": r[1],
            "skill_name": r[2],
            "note_type": r[3],
            "item_index": r[4],
            "item_text": r[5],
            "memo_text": r[6],
            "updated_at": r[7],
        })
    return out


# ============================
# Text Similarity (local) + caching
# ============================
@st.cache_resource(show_spinner=False)
def build_similarity_index_cached(corpus: Tuple[str, ...]) -> Tuple[TfidfVectorizer, Any]:
    vectorizer = TfidfVectorizer(stop_words=None, max_features=5000)
    X = vectorizer.fit_transform(list(corpus))
    return vectorizer, X


def get_similar_entries(df: pd.DataFrame, target_text: str, top_k: int = 5) -> List[Tuple[str, float]]:
    if top_k <= 0 or df.empty:
        return []
    corpus_list = df["raw_text"].fillna("").tolist()
    if len(corpus_list) < 2:
        return []

    corpus = tuple(corpus_list)
    vectorizer, X = build_similarity_index_cached(corpus)
    try:
        x_target = vectorizer.transform([target_text])
        sims = cosine_similarity(x_target, X).flatten()
    except Exception:
        return []

    pairs = list(zip(df["id"].tolist(), sims.tolist()))
    pairs.sort(key=lambda x: x[1], reverse=True)
    return pairs[:top_k]


# ============================
# Robust JSON parsing
# ============================
def strip_code_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^\s*```(?:json)?\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\s*```\s*$", "", s)
    return s.strip()


def _extract_first_json_object(s: str) -> str:
    s = strip_code_fences(s)
    start = s.find("{")
    if start == -1:
        return s

    depth = 0
    for i in range(start, len(s)):
        if s[i] == "{":
            depth += 1
        elif s[i] == "}":
            depth -= 1
            if depth == 0:
                return s[start:i + 1]
    return s


def _json_repair_minimal(s: str) -> str:
    s = s.strip()
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = re.sub(r",\s*([}\]])", r"\1", s)
    s = re.sub(r"\bTrue\b", "true", s)
    s = re.sub(r"\bFalse\b", "false", s)
    s = re.sub(r"\bNone\b", "null", s)
    return s


def robust_json_loads(s: str) -> Dict[str, Any]:
    raw = _extract_first_json_object(s)
    try:
        out = json.loads(raw)
    except Exception:
        out = json.loads(_json_repair_minimal(raw))
    if not isinstance(out, dict):
        raise ValueError("JSON ìµœìƒìœ„ê°€ ê°ì²´(dict)ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    return out


# ============================
# Analysis engines
# (ìš”ì•½ ì—†ìŒ / íŒ¨í„´ìš”ì•½ ì—†ìŒ / STAR ì—†ìŒ)
# ìƒí™©ë¶„ì„: í–‰ë™, ë°°ì›€
# ì„±ì¥í”Œëœ: top ìŠ¤í‚¬ ê¸°ì¤€ 2 practice + 2 question
# ============================
def get_openai_client(api_key: str):
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ë²„ì „ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. `pip install -U openai` í•´ì£¼ì„¸ìš”.")
    if not api_key or not api_key.strip():
        raise RuntimeError("OpenAI API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return OpenAI(api_key=api_key)


def analyze_entry_with_openai(
    api_key: str,
    model: str,
    entry: Dict[str, Any],
    related_summaries: List[Dict[str, Any]],
    output_mode: str = "analysis_only",
) -> Dict[str, Any]:
    client = get_openai_client(api_key)

    persona = (
        "ë‹¹ì‹ ì€ MetaToneì˜ ì½”ì¹˜ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ê¸°ë¡ì—ì„œ 'í–‰ë™'ê³¼ 'ë°°ì›€'ì„ ë½‘ê³ , ê·¸ ê¸°ë¡ì—ì„œ ë“œëŸ¬ë‚œ ì†Œí”„íŠ¸ìŠ¤í‚¬(1~3ê°œ)ì„ ê·¼ê±° ì¸ìš©ê³¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤. "
        "ê³¼ì¥/ë¯¸ì‚¬ì—¬êµ¬/ë‹¨ì • ê¸ˆì§€. ê·¼ê±° ì¤‘ì‹¬."
    )

    related_block = []
    for rs in (related_summaries or [])[:5]:
        related_block.append({
            "id": rs.get("id"),
            "date": rs.get("entry_date"),
            "one_liner": rs.get("one_liner", ""),
            "skills": rs.get("skills", []),
        })

    # JSON ê³„ì•½(ìš”ì•½ ì—†ìŒ, í–‰ë™/ë°°ì›€ë§Œ)
    output_contract: Dict[str, Any] = {
        "meta": {
            "entry_id": entry["id"],
            "entry_date": entry["entry_date"],
            "category": entry.get("category") or ""
        },
        "situation_analysis": {
            "actions": ["ë‚´ê°€ ì‹¤ì œë¡œ í•œ í–‰ë™ 2~4ê°œ(ì§§ì€ ë¬¸ì¥)"],
            "learnings": ["ë°°ì›€ 1~2ê°œ(ì§§ì€ ë¬¸ì¥)"]
        },
        "soft_skills": [
            {
                "name": "í˜‘ì—…",
                "confidence": 0.0,
                "evidence_quotes": ["ì›ë¬¸ ê·¸ëŒ€ë¡œ 1~2ê°œ(ê° 80ì ì´ë‚´)"],
                "why_it_counts": "ì™œ ì´ ì—­ëŸ‰ì¸ì§€ 1ë¬¸ì¥",
                "concept": "ê°œë… 1ë¬¸ì¥"
            }
        ],
        "growth_plan": {
            "top_skill": "í˜‘ì—…",
            "practices": ["ì—°ìŠµ/ë£¨í‹´ 1", "ì—°ìŠµ/ë£¨í‹´ 2"],
            "questions": ["ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸ 1", "ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸ 2"]
        }
    }

    user_payload = {
        "entry": {
            "entry_date": entry["entry_date"],
            "category": entry.get("category"),
            "raw_text": entry["raw_text"],
            "artifacts": entry.get("artifacts") or []
        },
        "related_entries_hint": related_block,
        "soft_skill_candidates": SOFT_SKILLS,
        "skill_concepts": SKILL_CONCEPTS,
        "output_contract_example": output_contract,
        "constraints": {
            "practice_n": PRACTICE_N,
            "question_n": QUESTION_N
        }
    }

    instructions = (
        "ê·œì¹™:\n"
        "1) ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥(ë§ˆí¬ë‹¤ìš´/ì½”ë“œíœìŠ¤/ì„¤ëª…ë¬¸ ê¸ˆì§€)\n"
        "2) soft_skillsëŠ” 1~3ê°œ, confidenceëŠ” 0~1 ìˆ«ì\n"
        "3) evidence_quotesëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìµœëŒ€ 2ê°œ, ê° 80ì ì´ë‚´\n"
        "4) situation_analysisëŠ” actions/learningsë§Œ (ìš”ì•½ ê¸ˆì§€)\n"
        f"5) growth_planì˜ practicesëŠ” ì •í™•íˆ {PRACTICE_N}ê°œ, questionsëŠ” ì •í™•íˆ {QUESTION_N}ê°œ\n"
        "6) growth_plan.top_skillì€ soft_skills ì¤‘ confidenceê°€ ê°€ì¥ ë†’ì€ ìŠ¤í‚¬ëª…\n"
        "7) conceptëŠ” skill_conceptsë¥¼ ì°¸ê³ í•´ 1ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ\n"
        "8) ê³¼ì¥/ë¯¸ì‚¬ì—¬êµ¬/ë‹¨ì • ê¸ˆì§€\n"
    )

    try:
        resp = client.chat.completions.create(
            model=model,
            temperature=0.4,
            messages=[
                {"role": "system", "content": persona},
                {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                {"role": "user", "content": instructions},
            ]
        )
    except Exception as e:
        raise RuntimeError(
            f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}\n\n"
            f"ì ê²€:\n- API Key ìœ íš¨ ì—¬ë¶€\n- ëª¨ë¸({model}) ì ‘ê·¼ ê¶Œí•œ/ì´ë¦„\n- ì‚¬ìš©ëŸ‰/ì¿¼í„°/ê²°ì œ ìƒíƒœ"
        )

    out = robust_json_loads(resp.choices[0].message.content or "")
    return out


def analyze_entry_local(
    entry: Dict[str, Any],
    related_summaries: List[Dict[str, Any]],
    output_mode: str = "analysis_only",
) -> Dict[str, Any]:
    text = (entry.get("raw_text") or "").strip()

    # í–‰ë™/ë°°ì›€: ë¬¸ì¥/ì¤„ì—ì„œ ê°„ë‹¨ ì¶”ì¶œ(ë³´ìˆ˜ì )
    lines = [l.strip() for l in re.split(r"[\n\r]+", text) if l.strip()]
    sentences = [s.strip() for s in re.split(r"[.!?\n]", text) if s.strip()]

    # í–‰ë™ í›„ë³´: ë™ì‚¬/í‘œí˜„ ê¸°ë°˜
    action_markers = ["í–ˆë‹¤", "í•¨", "ì§„í–‰", "ì •ë¦¬", "ê³µìœ ", "ì„¤ëª…", "ì¡°ìœ¨", "í™•ì¸", "ê°œì„ ", "ì‹œë„", "ê²°ì •", "ë¶„ì„", "ì œì•ˆ", "ìš”ì²­"]
    actions: List[str] = []
    for l in lines:
        if any(m in l for m in action_markers):
            actions.append(l[:140])
        if len(actions) >= 4:
            break
    if not actions:
        # fallback: ì• ë¬¸ì¥ ì¼ë¶€
        actions = [sentences[0][:140]] if sentences else ["(í–‰ë™) ë‚´ê°€ ì‹¤ì œë¡œ í•œ ì¼ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì ì–´ë³´ì„¸ìš”."]

    learning_markers = ["ë°°ì› ", "ê¹¨ë‹¬", "ë‹¤ìŒ", "ê°œì„ ", "ë°˜ì„±", "ëŠê¼ˆ", "ì•Œê²Œ", "êµí›ˆ", "ì„±ì°°"]
    learnings: List[str] = []
    for l in reversed(lines):
        if any(m in l for m in learning_markers):
            learnings.append(l[:140])
        if len(learnings) >= 2:
            break
    learnings = list(reversed(learnings))
    if not learnings:
        learnings = ["(ë°°ì›€) ì˜¤ëŠ˜ ì–»ì€ êµí›ˆ/ë‹¤ìŒ ê¸°ì¤€ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê²¨ë³´ì„¸ìš”."]

    # ìŠ¤í‚¬ ë£°
    skill_rules = {
        "ë¬¸ì œí•´ê²°": ["ë¬¸ì œ", "ì›ì¸", "í•´ê²°", "ë¶„ì„", "ë””ë²„ê¹…", "êµ¬ì¡°", "ëŒ€ì•ˆ", "ê°œì„ "],
        "ì˜ì‚¬ì†Œí†µ": ["ì„¤ëª…", "ê³µìœ ", "ë°œí‘œ", "ì„¤ë“", "ì •ë¦¬", "ë¬¸ì„œ", "í”¼ë“œë°±", "í•©ì˜"],
        "í˜‘ì—…": ["íŒ€", "í˜‘ì—…", "ì¡°ìœ¨", "ì—­í• ", "íšŒì˜", "ê°ˆë“±", "ë™ë£Œ", "í•¨ê»˜"],
        "ë¦¬ë”ì‹­": ["ì£¼ë„", "ë¦¬ë“œ", "ê²°ì •", "ë°©í–¥", "ê°€ì´ë“œ", "ì½”ì¹­", "ì±…ì„", "ê¸°íš"],
        "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": ["ì‹œê°„", "ë£¨í‹´", "íšŒë³µ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì••ë°•", "ìš°ì„ ìˆœìœ„", "ì§€ì†", "ì»¨ë””ì…˜"],
        "í•™ìŠµì—­ëŸ‰": ["ê³µë¶€", "í•™ìŠµ", "ì •ë¦¬", "ë³µìŠµ", "ì‹¤í—˜", "ê°œë…", "ê°•ì˜", "ë…ì„œ", "ì—°ìŠµ"],
    }

    scores = {k: 0 for k in SOFT_SKILLS}
    for sk, kws in skill_rules.items():
        for kw in kws:
            if kw in text:
                scores[sk] += 1

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    picked = [(k, v) for k, v in ranked if v > 0][:3]
    if not picked:
        picked = [("í•™ìŠµì—­ëŸ‰", 1)]

    evidence = {k: [] for k in SOFT_SKILLS}
    for sk, _v in picked:
        kws = skill_rules.get(sk, [])
        for s in sentences:
            if any(kw in s for kw in kws):
                evidence[sk].append(s[:80])
                if len(evidence[sk]) >= 2:
                    break

    max_score = max(v for _, v in picked) if picked else 1
    soft_skills = []
    for sk, v in picked:
        conf = 0.4 + 0.6 * (v / max_score) if max_score > 0 else 0.5
        soft_skills.append({
            "name": sk,
            "confidence": round(min(max(conf, 0.0), 1.0), 2),
            "evidence_quotes": evidence[sk][:2] if evidence[sk] else (sentences[:1] if sentences else []),
            "why_it_counts": "ì›ë¬¸ì—ì„œ í•´ë‹¹ í–‰ë™ ë‹¨ì„œ(í‚¤ì›Œë“œ/í‘œí˜„)ê°€ ë³´ì—¬ ì´ ì—­ëŸ‰ì´ ë“œëŸ¬ë‚œ ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. (ë¬´ë£Œ ë¡œì»¬ ë¶„ì„)",
            "concept": SKILL_CONCEPTS.get(sk, "")
        })

    # top_skill = confidence max
    top_skill = soft_skills[0]["name"]
    # growth_plan: top skill ê¸°ì¤€ 2+2
    practices = [
        "ë‹¤ìŒ ê¸°ë¡ì—ì„œ 'ë‚´ê°€ ì„ íƒí•œ ê¸°ì¤€(ìš°ì„ ìˆœìœ„/ê·¼ê±°)'ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê¸°ê¸°",
        "ê²°ê³¼ë¥¼ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„(ì „/í›„ ë³€í™”, ì‹œê°„/íšŸìˆ˜/í’ˆì§ˆ)ë¡œ ì ê¸°",
    ][:PRACTICE_N]
    questions = [
        "ë‚´ê°€ í•œ ì„ íƒì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì´ì—ˆë‚˜?",
        "ë‹¤ìŒì— ê°™ì€ ìƒí™©ì´ë©´ ë¬´ì—‡ì„ ìœ ì§€/ë³€ê²½í• ê¹Œ?",
    ][:QUESTION_N]

    out: Dict[str, Any] = {
        "meta": {
            "entry_id": entry["id"],
            "entry_date": entry["entry_date"],
            "category": entry.get("category") or ""
        },
        "situation_analysis": {
            "actions": actions[:4],
            "learnings": learnings[:2],
        },
        "soft_skills": soft_skills,
        "growth_plan": {
            "top_skill": top_skill,
            "practices": practices,
            "questions": questions
        }
    }
    return out


def run_analysis_engine(
    engine: str,
    entry: Dict[str, Any],
    related: List[Dict[str, Any]],
) -> Dict[str, Any]:
    if engine.startswith("ë¬´ë£Œ"):
        return analyze_entry_local(entry=entry, related_summaries=related)

    api_key = st.session_state.get("api_key", "")
    if not api_key:
        st.warning("LLM ë¶„ì„ì„ ì„ íƒí–ˆì§€ë§Œ API Keyê°€ ì—†ì–´ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return analyze_entry_local(entry=entry, related_summaries=related)

    try:
        return analyze_entry_with_openai(
            api_key=api_key,
            model=st.session_state.get("model", DEFAULT_MODEL),
            entry=entry,
            related_summaries=related
        )
    except Exception as e:
        st.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨ â†’ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n\nì‚¬ìœ : {e}")
        return analyze_entry_local(entry=entry, related_summaries=related)


# ============================
# Aggregations / Related summaries
# ============================
def compute_skill_totals(df: pd.DataFrame) -> Dict[str, int]:
    totals = {s: 0 for s in SOFT_SKILLS}
    if df.empty:
        return totals
    for an in df["analysis_parsed"].tolist():
        if not isinstance(an, dict):
            continue
        skills = an.get("soft_skills") or []
        if not isinstance(skills, list):
            continue
        for sk in skills:
            if isinstance(sk, dict):
                name = sk.get("name")
                if name in totals:
                    totals[name] += 1
    return totals


def summarize_for_related(df: pd.DataFrame) -> List[Dict[str, Any]]:
    summaries: List[Dict[str, Any]] = []
    for _, r in df.iterrows():
        an = r.get("analysis_parsed") or {}
        one_liner = ""
        skills: List[str] = []
        try:
            sa = (an.get("situation_analysis", {}) or {}) if isinstance(an, dict) else {}
            learnings = sa.get("learnings") or []
            if isinstance(learnings, list) and learnings:
                one_liner = (learnings[0] or "")[:80]
            soft = (an.get("soft_skills") or []) if isinstance(an, dict) else []
            skills = [x.get("name") for x in soft if isinstance(x, dict) and x.get("name")]
        except Exception:
            pass
        summaries.append({
            "id": r["id"],
            "entry_date": r["entry_date"],
            "one_liner": one_liner,
            "skills": skills
        })
    return summaries


# ============================
# Notes initialization + UI helpers
# ============================
def ensure_notes_initialized(
    entry_id: str,
    entry_date: str,
    skill_name: str,
    practices: List[str],
    questions: List[str],
) -> None:
    """
    notes í…Œì´ë¸”ì— ê¸°ë³¸ rowê°€ ì—†ìœ¼ë©´ ë§Œë“¤ì–´ë‘”ë‹¤.
    (ì´ë¯¸ ìˆìœ¼ë©´ upsertë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ: ì‚¬ìš©ìê°€ ìˆ˜ì •í•œ í…ìŠ¤íŠ¸/ë©”ëª¨ë¥¼ ë³´í˜¸)
    """
    existing = fetch_skill_notes_for_entry(entry_id)
    exists_keys = set()
    for n in existing:
        exists_keys.add((n["skill_name"], n["note_type"], int(n["item_index"])))

    now = datetime.now().isoformat(timespec="seconds")
    with get_conn() as conn:
        cur = conn.cursor()
        # practices
        for i in range(PRACTICE_N):
            key = (skill_name, "practice", i)
            if key in exists_keys:
                continue
            item_text = practices[i] if i < len(practices) else ""
            cur.execute("""
                INSERT OR IGNORE INTO skill_notes
                (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                str(uuid.uuid4()), entry_id, entry_date, skill_name, "practice", i,
                item_text, "", now, now
            ))

        # questions
        for i in range(QUESTION_N):
            key = (skill_name, "question", i)
            if key in exists_keys:
                continue
            item_text = questions[i] if i < len(questions) else ""
            cur.execute("""
                INSERT OR IGNORE INTO skill_notes
                (id, entry_id, entry_date, skill_name, note_type, item_index, item_text, memo_text, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                str(uuid.uuid4()), entry_id, entry_date, skill_name, "question", i,
                item_text, "", now, now
            ))
        conn.commit()


def group_notes(notes: List[Dict[str, Any]]) -> Dict[Tuple[str, str], Dict[int, Dict[str, str]]]:
    """
    return: {(skill_name, note_type): {idx: {"item_text":..., "memo_text":...}}}
    """
    out: Dict[Tuple[str, str], Dict[int, Dict[str, str]]] = {}
    for n in notes:
        k = (n["skill_name"], n["note_type"])
        out.setdefault(k, {})
        out[k][int(n["item_index"])] = {
            "item_text": n.get("item_text") or "",
            "memo_text": n.get("memo_text") or ""
        }
    return out


def render_skill_totals(totals: Dict[str, int]) -> None:
    st.subheader("ğŸ“ˆ ì†Œí”„íŠ¸ìŠ¤í‚¬ ëˆ„ì (ë²”ì£¼ë³„)")
    cols = st.columns(3)
    items = list(totals.items())
    for i, (k, v) in enumerate(items):
        with cols[i % 3]:
            st.metric(label=k, value=v)

    df_tot = (
        pd.DataFrame([{"soft_skill": k, "count": v} for k, v in totals.items()])
        .sort_values("count", ascending=False)
    )
    st.dataframe(df_tot, use_container_width=True, hide_index=True)


def render_analysis_block(analysis: Dict[str, Any]) -> None:
    if not analysis or not isinstance(analysis, dict):
        st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ğŸ§  ìƒí™©ë¶„ì„")
    sa = analysis.get("situation_analysis", {}) or {}
    actions = sa.get("actions") or []
    learnings = sa.get("learnings") or []

    st.markdown("**í–‰ë™**")
    if isinstance(actions, list) and actions:
        for a in actions:
            st.write(f"- {a}")
    else:
        st.write("â€”")

    st.markdown("**ë°°ì›€**")
    if isinstance(learnings, list) and learnings:
        for l in learnings:
            st.write(f"- {l}")
    else:
        st.write("â€”")

    st.subheader("ğŸ¯ ì˜¤ëŠ˜ ìŒ“ì€ ì†Œí”„íŠ¸ ìŠ¤í‚¬")
    skills = analysis.get("soft_skills", []) or []
    if not isinstance(skills, list) or not skills:
        st.write("ì„ ì •ëœ ì—­ëŸ‰ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for sk in skills:
        if not isinstance(sk, dict):
            continue
        name = sk.get("name", "")
        conf = sk.get("confidence", 0)
        try:
            conf = float(conf)
        except Exception:
            conf = 0.0

        with st.expander(f"{name} (confidence: {conf:.2f})", expanded=False):
            ev = sk.get("evidence_quotes", []) or []
            if isinstance(ev, list) and ev:
                st.markdown("**ê·¼ê±°(ì›ë¬¸ ì¸ìš©)**")
                for q in ev[:2]:
                    st.caption(f"â€œ{q}â€")

            st.markdown("**ì™œ ì´ ìŠ¤í‚¬ì¸ê°€**")
            st.write(sk.get("why_it_counts", ""))

            st.markdown("**ê°œë… ì„¤ëª…**")
            st.info(sk.get("concept") or SKILL_CONCEPTS.get(name, ""))


def get_top_skill_from_analysis(analysis: Dict[str, Any]) -> Optional[str]:
    if not isinstance(analysis, dict):
        return None
    gp = analysis.get("growth_plan", {}) or {}
    top = gp.get("top_skill")
    if top in SOFT_SKILLS:
        return top

    # fallback: soft_skills[0]
    skills = analysis.get("soft_skills") or []
    if isinstance(skills, list) and skills and isinstance(skills[0], dict):
        name = skills[0].get("name")
        if name in SOFT_SKILLS:
            return name
    return None


def get_growth_items_for_top_skill(analysis: Dict[str, Any]) -> Tuple[List[str], List[str]]:
    gp = (analysis.get("growth_plan", {}) or {}) if isinstance(analysis, dict) else {}
    practices = gp.get("practices") or []
    questions = gp.get("questions") or []
    if not isinstance(practices, list):
        practices = []
    if not isinstance(questions, list):
        questions = []
    # ì •í™•íˆ 2ê°œë¡œ ë§ì¶”ê¸°(ë¶€ì¡±í•˜ë©´ ë¹ˆ ê°’)
    practices = (practices + [""] * PRACTICE_N)[:PRACTICE_N]
    questions = (questions + [""] * QUESTION_N)[:QUESTION_N]
    return practices, questions


def render_memo_editor_for_skill(
    entry_id: str,
    entry_date: str,
    skill_name: str,
    default_practices: List[str],
    default_questions: List[str],
) -> None:
    """
    - item_text ìˆ˜ì • ê°€ëŠ¥
    - memo_text ì…ë ¥ ê°€ëŠ¥
    - ì €ì¥ ë²„íŠ¼ìœ¼ë¡œ upsert
    """
    ensure_notes_initialized(entry_id, entry_date, skill_name, default_practices, default_questions)
    notes = fetch_skill_notes_for_entry(entry_id)
    grouped = group_notes(notes)

    st.markdown(f"### âœï¸ ë©”ëª¨ â€” {skill_name}")
    st.caption("ì—°ìŠµ/ì§ˆë¬¸ ë¬¸êµ¬ë„ ìˆ˜ì •í•  ìˆ˜ ìˆì–´ìš”. ì €ì¥ì„ ëˆŒëŸ¬ ë°˜ì˜í•˜ì„¸ìš”.")

    # practices
    st.markdown("**ì—°ìŠµ/ë£¨í‹´ (2)**")
    for i in range(PRACTICE_N):
        cur = grouped.get((skill_name, "practice"), {}).get(i, {"item_text": "", "memo_text": ""})
        item_key = f"item_{entry_id}_{skill_name}_practice_{i}"
        memo_key = f"memo_{entry_id}_{skill_name}_practice_{i}"

        st.text_input(f"ì—°ìŠµ {i+1}", value=cur["item_text"], key=item_key)
        st.text_area("ë©”ëª¨", value=cur["memo_text"], key=memo_key, height=80)

    # questions
    st.markdown("**ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸ (2)**")
    for i in range(QUESTION_N):
        cur = grouped.get((skill_name, "question"), {}).get(i, {"item_text": "", "memo_text": ""})
        item_key = f"item_{entry_id}_{skill_name}_question_{i}"
        memo_key = f"memo_{entry_id}_{skill_name}_question_{i}"

        st.text_input(f"ì§ˆë¬¸ {i+1}", value=cur["item_text"], key=item_key)
        st.text_area("ë©”ëª¨", value=cur["memo_text"], key=memo_key, height=80)

    if st.button("ğŸ’¾ ë©”ëª¨ ì €ì¥", key=f"save_{entry_id}_{skill_name}"):
        # practices save
        for i in range(PRACTICE_N):
            item_key = f"item_{entry_id}_{skill_name}_practice_{i}"
            memo_key = f"memo_{entry_id}_{skill_name}_practice_{i}"
            upsert_skill_note(
                entry_id=entry_id,
                entry_date=entry_date,
                skill_name=skill_name,
                note_type="practice",
                item_index=i,
                item_text=st.session_state.get(item_key, ""),
                memo_text=st.session_state.get(memo_key, ""),
            )
        # questions save
        for i in range(QUESTION_N):
            item_key = f"item_{entry_id}_{skill_name}_question_{i}"
            memo_key = f"memo_{entry_id}_{skill_name}_question_{i}"
            upsert_skill_note(
                entry_id=entry_id,
                entry_date=entry_date,
                skill_name=skill_name,
                note_type="question",
                item_index=i,
                item_text=st.session_state.get(item_key, ""),
                memo_text=st.session_state.get(memo_key, ""),
            )
        st.success("ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# ============================
# Streamlit Pages
# ============================
def main():
    st.set_page_config(page_title="MetaTone", layout="wide")
    init_db()

    # Sidebar settings
    st.sidebar.title("âš™ï¸ Settings")
    api_key_env = os.getenv("OPENAI_API_KEY", "")
    api_key_input = st.sidebar.text_input(
        "OpenAI API Key (ì„ íƒ)",
        value=st.session_state.get("api_key", api_key_env),
        type="password"
    )
    st.session_state["api_key"] = (api_key_input or "").strip()

    current_model = st.session_state.get("model", DEFAULT_MODEL)
    if current_model not in MODEL_OPTIONS:
        current_model = DEFAULT_MODEL
    st.sidebar.selectbox(
        "Model (LLM ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)",
        options=MODEL_OPTIONS,
        index=MODEL_OPTIONS.index(current_model),
        key="model"
    )

    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ§  ë¶„ì„ ì—”ì§„")
    st.sidebar.selectbox("ë¶„ì„ ë°©ì‹", options=ANALYSIS_ENGINES, index=0, key="engine")

    st.sidebar.markdown("---")
    page = st.sidebar.radio("í˜ì´ì§€", ["âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€", "ğŸ“š ê¸°ë¡ ëª©ë¡", "ğŸ“’ ë©”ëª¨", "ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸"])

    df = fetch_entries()
    totals = compute_skill_totals(df)

    st.title(APP_TITLE)
    st.caption("ê¸°ë¡ ë³¸ë¬¸ì—ì„œ í–‰ë™/ë°°ì›€ì„ ë½‘ê³ , ì˜¤ëŠ˜ ë“œëŸ¬ë‚œ ì†Œí”„íŠ¸ìŠ¤í‚¬ê³¼ (top ìŠ¤í‚¬ ê¸°ì¤€) 2+2 ë£¨í‹´/ì§ˆë¬¸ ë©”ëª¨ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.")

    render_skill_totals(totals)
    st.markdown("---")

    if page == "âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€":
        render_new_entry(df)
    elif page == "ğŸ“š ê¸°ë¡ ëª©ë¡":
        render_history(df)
    elif page == "ğŸ“’ ë©”ëª¨":
        render_notes_page(df)
    else:
        render_debug(df)


def render_new_entry(df: pd.DataFrame):
    st.subheader("âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€")

    st.info("í•œ ë°•ìŠ¤ì— ììœ ë¡­ê²Œ ì ë˜, ê°€ëŠ¥í•˜ë©´ **í–‰ë™ â†’ ê²½í—˜í•œ ê°ì • â†’ ê²°ê³¼** ìˆœì„œë¡œ ì¨ë³´ì„¸ìš”.\n"
            "ì˜ˆ) ì˜¤ëŠ˜ ë‚´ê°€ í•œ í–‰ë™ / ê·¸ë•Œ ëŠë‚€ ê°ì • / ê²°ê³¼ì™€ ë°°ì›€")

    col1, col2 = st.columns([1, 1])
    with col1:
        entry_date = st.date_input("ë‚ ì§œ", value=date.today())
        cat_choice = st.selectbox(
            "ì¹´í…Œê³ ë¦¬(ì„ íƒ)",
            options=["(ì„ íƒ ì•ˆ í•¨)"] + CATEGORIES + ["(ì§ì ‘ ì…ë ¥)"],
            index=0
        )
        cat_custom = ""
        if cat_choice == "(ì§ì ‘ ì…ë ¥)":
            cat_custom = st.text_input("ì¹´í…Œê³ ë¦¬ ì§ì ‘ ì…ë ¥", placeholder="ì˜ˆ: ì¸í„´/í˜„ì¥ì‹¤ìŠµ, ê°œì¸ í”„ë¡œì íŠ¸, ì·¨ë¯¸ í™œë™ ë“±")
        category = None
        if cat_choice == "(ì„ íƒ ì•ˆ í•¨)":
            category = None
        elif cat_choice == "(ì§ì ‘ ì…ë ¥)":
            category = cat_custom.strip() if cat_custom.strip() else None
        else:
            category = cat_choice

    with col2:
        artifacts = st.text_area(
            "ì¦ê±°/ìë£Œ ë§í¬(ì„ íƒ) â€” ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ",
            placeholder="ì˜ˆ: Notion ë§í¬, Google Doc, GitHub, ë°œí‘œìë£Œ URL ë“±"
        )
        artifacts_list = [x.strip() for x in (artifacts or "").splitlines() if x.strip()]

    raw_text = st.text_area(
        "ê¸°ë¡ ë³¸ë¬¸(í•„ìˆ˜)",
        height=260,
        placeholder="í–‰ë™ â†’ ê°ì • â†’ ê²°ê³¼ ìˆœì„œë¡œ ì ì–´ë³´ì„¸ìš”.\n(ì˜ˆ: ë‚´ê°€ í•œ í–‰ë™ / ëŠë‚€ ê°ì • / ê²°ê³¼ + ë°°ì›€)"
    )

    st.markdown("### ğŸ” ë¶„ì„ ì˜µì…˜")
    do_analysis = st.checkbox("ì €ì¥ í›„ ë¶„ì„ ì‹¤í–‰í•˜ê¸°", value=True)
    top_k = st.slider("ìœ ì‚¬ ê¸°ë¡ íŒíŠ¸(top-k)", min_value=0, max_value=10, value=5)

    if st.button("âœ… ì €ì¥", type="primary"):
        if not (raw_text or "").strip():
            st.error("ê¸°ë¡ ë³¸ë¬¸ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
            return

        entry_id = str(uuid.uuid4())
        created_at = datetime.now().isoformat(timespec="seconds")
        entry = {
            "id": entry_id,
            "created_at": created_at,
            "entry_date": entry_date.isoformat(),
            "category": category,
            "tags": [],
            "title": None,
            "raw_text": raw_text.strip(),
            "artifacts": artifacts_list,
            "analysis": {}
        }
        insert_entry(entry)
        st.success("ê¸°ë¡ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        if not do_analysis:
            return

        # related hints (optional)
        related: List[Dict[str, Any]] = []
        if top_k > 0 and not df.empty:
            sims = get_similar_entries(df, entry["raw_text"], top_k=top_k)
            hint_rows: List[Dict[str, Any]] = []
            for rid, _score in sims:
                r = fetch_entry_by_id(rid)
                if r:
                    hint_rows.append(r)
            hint_df = pd.DataFrame(hint_rows) if hint_rows else pd.DataFrame()
            if not hint_df.empty:
                hint_df["analysis_parsed"] = hint_df["analysis_json"].apply(
                    lambda x: x if isinstance(x, dict) else (x or {})
                )
                related = summarize_for_related(hint_df)

        engine = st.session_state.get("engine", DEFAULT_ENGINE)
        with st.spinner("ë¶„ì„ ì¤‘..."):
            analysis = run_analysis_engine(engine=engine, entry=entry, related=related)
            update_entry_analysis(entry_id, analysis)

        st.success("ë¶„ì„ ì™„ë£Œ!")
        render_analysis_block(analysis)

        # top skill memo editor (ê¸°ë³¸: top 1ê°œë§Œ)
        top_skill = get_top_skill_from_analysis(analysis)
        if top_skill:
            practices, questions = get_growth_items_for_top_skill(analysis)
            st.markdown("---")
            render_memo_editor_for_skill(
                entry_id=entry_id,
                entry_date=entry["entry_date"],
                skill_name=top_skill,
                default_practices=practices,
                default_questions=questions
            )

            # ì˜µì…˜: ë‹¤ë¥¸ ìŠ¤í‚¬ë„ ë©”ëª¨í•˜ê¸°
            other_skills = []
            skills = analysis.get("soft_skills") or []
            if isinstance(skills, list):
                for sk in skills:
                    if isinstance(sk, dict) and sk.get("name") and sk.get("name") != top_skill:
                        other_skills.append(sk.get("name"))
            if other_skills:
                if st.toggle("ë‹¤ë¥¸ ìŠ¤í‚¬ë„ ë©”ëª¨í•˜ê¸°", value=False, key=f"toggle_other_{entry_id}"):
                    st.markdown("---")
                    st.subheader("â• ë‹¤ë¥¸ ìŠ¤í‚¬ ë©”ëª¨")
                    st.caption("ë‹¤ë¥¸ ìŠ¤í‚¬ì€ ê¸°ë³¸ í…œí”Œë¦¿(2+2)ë¡œ ì‹œì‘í•˜ë©°, ë¬¸êµ¬/ë©”ëª¨ ëª¨ë‘ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                    for osk in other_skills:
                        default_pr = [
                            f"{osk}ì„(ë¥¼) ê°•í™”í•˜ê¸° ìœ„í•´, ë‹¤ìŒ ê¸°ë¡ì— 'ë‚´ í–‰ë™'ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ 1ë¬¸ì¥ ì¶”ê°€í•˜ê¸°",
                            f"{osk} ê´€ë ¨ ê²°ê³¼ë¥¼ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„ìœ¼ë¡œ 1ë¬¸ì¥ ì¶”ê°€í•˜ê¸°",
                        ][:PRACTICE_N]
                        default_qs = [
                            f"ì˜¤ëŠ˜ {osk} ê´€ì ì—ì„œ ë‚´ê°€ í•œ ì„ íƒì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì´ì—ˆë‚˜?",
                            f"ë‹¤ìŒì—” {osk} ê´€ì ì—ì„œ ë¬´ì—‡ì„ ë°”ê¾¸ë©´ ë” ì¢‹ì•„ì§ˆê¹Œ?",
                        ][:QUESTION_N]
                        render_memo_editor_for_skill(
                            entry_id=entry_id,
                            entry_date=entry["entry_date"],
                            skill_name=osk,
                            default_practices=default_pr,
                            default_questions=default_qs
                        )
        else:
            st.info("top ìŠ¤í‚¬ì„ ê²°ì •í•  ìˆ˜ ì—†ì–´ ë©”ëª¨ ì„¹ì…˜ì„ í‘œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ë¶„ì„ ê²°ê³¼ì— soft_skillsê°€ í•„ìš”í•©ë‹ˆë‹¤.)")


def render_history(df: pd.DataFrame):
    st.subheader("ğŸ“š ê¸°ë¡ ëª©ë¡")
    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. 'ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€'ì—ì„œ ì‘ì„±í•´ë³´ì„¸ìš”.")
        return

    colf1, colf2, colf3 = st.columns([1, 1, 2])
    with colf1:
        cat = st.selectbox("ì¹´í…Œê³ ë¦¬ í•„í„°", options=["(ì „ì²´)"] + CATEGORIES)
    with colf2:
        skill_filter = st.selectbox("ì†Œí”„íŠ¸ìŠ¤í‚¬ í•„í„°", options=["(ì „ì²´)"] + SOFT_SKILLS)
    with colf3:
        q = st.text_input("ê²€ìƒ‰(ë³¸ë¬¸)", placeholder="ì˜ˆ: ë°œí‘œ, ì¡°ìœ¨, íšŒë³µ, ê¸°ì¤€, í”¼ë“œë°±...")

    filtered = df.copy()

    if cat != "(ì „ì²´)":
        filtered = filtered[filtered["category"] == cat]

    if (q or "").strip():
        qq = q.strip().lower()
        filtered = filtered[filtered["raw_text"].str.lower().str.contains(qq, na=False)]

    if skill_filter != "(ì „ì²´)":
        def has_skill(an):
            if not isinstance(an, dict):
                return False
            skills = an.get("soft_skills", []) or []
            return any((s.get("name") == skill_filter) for s in skills if isinstance(s, dict))
        filtered = filtered[filtered["analysis_parsed"].apply(has_skill)]

    st.caption(f"ì´ {len(filtered)}ê°œ")

    engine = st.session_state.get("engine", DEFAULT_ENGINE)

    for _, r in filtered.iterrows():
        entry_date = r.get("entry_date", "")
        category = r.get("category") or "â€”"
        an = r.get("analysis_parsed") or {}
        if not isinstance(an, dict):
            an = {}
        skills = [s.get("name") for s in (an.get("soft_skills") or []) if isinstance(s, dict) and s.get("name")]
        skill_text = ", ".join(skills) if skills else "â€”"

        with st.expander(f"{entry_date} Â· ì¹´í…Œê³ ë¦¬: {category}  |  ìŠ¤í‚¬: {skill_text}"):
            st.write(r["raw_text"])

            artifacts = r.get("artifacts_parsed") or []
            if artifacts:
                st.markdown("**ì¦ê±°/ë§í¬**")
                for a in artifacts:
                    st.write(f"- {a}")

            st.markdown("---")
            if an:
                render_analysis_block(an)
            else:
                st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ìš”.")

            colb1, colb2 = st.columns([1, 1])
            with colb1:
                if st.button("ğŸ¤– ì´ ê¸°ë¡ ë¶„ì„í•˜ê¸°", key=f"an_{r['id']}"):
                    entry = fetch_entry_by_id(r["id"])
                    if not entry:
                        st.error("ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        other = df[df["id"] != r["id"]].head(80)
                        related = summarize_for_related(other) if not other.empty else []
                        payload = {
                            "id": entry["id"],
                            "entry_date": entry["entry_date"],
                            "category": entry.get("category"),
                            "raw_text": entry["raw_text"],
                            "artifacts": entry.get("artifacts") or []
                        }
                        with st.spinner("ë¶„ì„ ì¤‘..."):
                            analysis = run_analysis_engine(engine=engine, entry=payload, related=related)
                            update_entry_analysis(r["id"], analysis)
                        st.success("ë¶„ì„ ì™„ë£Œ! í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                        st.rerun()

            with colb2:
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{r['id']}"):
                    delete_entry(r["id"])
                    st.success("ì‚­ì œí–ˆìŠµë‹ˆë‹¤. í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                    st.rerun()

            # Memo section (ë¶„ì„ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ)
            an_now = r.get("analysis_parsed") or {}
            if isinstance(an_now, dict) and an_now.get("soft_skills"):
                top_skill = get_top_skill_from_analysis(an_now)
                if top_skill:
                    practices, questions = get_growth_items_for_top_skill(an_now)
                    st.markdown("---")
                    render_memo_editor_for_skill(
                        entry_id=r["id"],
                        entry_date=r["entry_date"],
                        skill_name=top_skill,
                        default_practices=practices,
                        default_questions=questions
                    )

                    other_skills = []
                    skills = an_now.get("soft_skills") or []
                    if isinstance(skills, list):
                        for sk in skills:
                            if isinstance(sk, dict) and sk.get("name") and sk.get("name") != top_skill:
                                other_skills.append(sk.get("name"))

                    if other_skills:
                        if st.toggle("ë‹¤ë¥¸ ìŠ¤í‚¬ë„ ë©”ëª¨í•˜ê¸°", value=False, key=f"toggle_other_hist_{r['id']}"):
                            st.markdown("---")
                            st.subheader("â• ë‹¤ë¥¸ ìŠ¤í‚¬ ë©”ëª¨")
                            for osk in other_skills:
                                default_pr = [
                                    f"{osk}ì„(ë¥¼) ê°•í™”í•˜ê¸° ìœ„í•´, ë‹¤ìŒ ê¸°ë¡ì— 'ë‚´ í–‰ë™'ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ 1ë¬¸ì¥ ì¶”ê°€í•˜ê¸°",
                                    f"{osk} ê´€ë ¨ ê²°ê³¼ë¥¼ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„ìœ¼ë¡œ 1ë¬¸ì¥ ì¶”ê°€í•˜ê¸°",
                                ][:PRACTICE_N]
                                default_qs = [
                                    f"ì˜¤ëŠ˜ {osk} ê´€ì ì—ì„œ ë‚´ê°€ í•œ ì„ íƒì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì´ì—ˆë‚˜?",
                                    f"ë‹¤ìŒì—” {osk} ê´€ì ì—ì„œ ë¬´ì—‡ì„ ë°”ê¾¸ë©´ ë” ì¢‹ì•„ì§ˆê¹Œ?",
                                ][:QUESTION_N]
                                render_memo_editor_for_skill(
                                    entry_id=r["id"],
                                    entry_date=r["entry_date"],
                                    skill_name=osk,
                                    default_practices=default_pr,
                                    default_questions=default_qs
                                )


def render_notes_page(df: pd.DataFrame):
    st.subheader("ğŸ“’ ë©”ëª¨")
    st.caption("ë©”ëª¨ëŠ” ê¸°ë¡(entry) ê¸°ì¤€ìœ¼ë¡œë„, ì†Œí”„íŠ¸ìŠ¤í‚¬ ê¸°ì¤€ìœ¼ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.")

    tab1, tab2 = st.tabs(["ë‚ ì§œë³„", "ìŠ¤í‚¬ë³„"])

    with tab1:
        # ë‚ ì§œ ì„ íƒ: entriesì—ì„œ ë‚ ì§œ ëª©ë¡ ìƒì„±
        dates = sorted(df["entry_date"].dropna().unique().tolist(), reverse=True) if not df.empty else []
        if not dates:
            st.info("ì•„ì§ ê¸°ë¡/ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            d = st.selectbox("ë‚ ì§œ ì„ íƒ", options=dates, index=0)
            notes = fetch_skill_notes_by_date(d)
            if not notes:
                st.info("ì´ ë‚ ì§œì— ì €ì¥ëœ ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # entry_idë³„ ê·¸ë£¹
                by_entry: Dict[str, List[Dict[str, Any]]] = {}
                for n in notes:
                    by_entry.setdefault(n["entry_id"], []).append(n)

                for entry_id, ns in by_entry.items():
                    with st.expander(f"{d} Â· entry_id: {entry_id[:8]} Â· ë©”ëª¨ {len(ns)}ê°œ"):
                        # skillë³„ ê·¸ë£¹
                        by_skill: Dict[str, List[Dict[str, Any]]] = {}
                        for n in ns:
                            by_skill.setdefault(n["skill_name"], []).append(n)
                        for sk, sk_notes in by_skill.items():
                            st.markdown(f"**{sk}**")
                            # practice/question ë¶„ë¦¬ ì¶œë ¥
                            for nt in ["practice", "question"]:
                                items = [x for x in sk_notes if x["note_type"] == nt]
                                if not items:
                                    continue
                                st.caption("ì—°ìŠµ/ë£¨í‹´" if nt == "practice" else "ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸")
                                for it in sorted(items, key=lambda x: int(x["item_index"])):
                                    st.write(f"- {it['item_text']}")
                                    if (it.get("memo_text") or "").strip():
                                        st.write(f"  â†³ ë©”ëª¨: {it['memo_text']}")

    with tab2:
        skill = st.selectbox("ìŠ¤í‚¬ ì„ íƒ", options=SOFT_SKILLS, index=0)
        notes = fetch_skill_notes_by_skill(skill, limit=300)
        if not notes:
            st.info("ì´ ìŠ¤í‚¬ì— ì €ì¥ëœ ë©”ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # entry_dateë³„ ê·¸ë£¹
            by_date: Dict[str, List[Dict[str, Any]]] = {}
            for n in notes:
                by_date.setdefault(n["entry_date"], []).append(n)

            for d in sorted(by_date.keys(), reverse=True):
                with st.expander(f"{d} Â· ë©”ëª¨ {len(by_date[d])}ê°œ"):
                    items = by_date[d]
                    for nt in ["practice", "question"]:
                        sub = [x for x in items if x["note_type"] == nt]
                        if not sub:
                            continue
                        st.caption("ì—°ìŠµ/ë£¨í‹´" if nt == "practice" else "ë‹¤ìŒ ê¸°ë¡ ì§ˆë¬¸")
                        for it in sorted(sub, key=lambda x: int(x["item_index"])):
                            st.write(f"- {it['item_text']}")
                            if (it.get("memo_text") or "").strip():
                                st.write(f"  â†³ ë©”ëª¨: {it['memo_text']}")
                            st.caption(f"entry_id: {it['entry_id'][:8]} Â· updated: {it['updated_at']}")


def render_debug(df: pd.DataFrame):
    st.subheader("ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸")
    st.write("í˜„ì¬ DBì— ì €ì¥ëœ ê¸°ë¡ ê°œìˆ˜:", len(df))

    st.markdown("### ìµœê·¼ 10ê°œ ê¸°ë¡ ë¯¸ë¦¬ë³´ê¸°")
    if not df.empty:
        st.dataframe(df[["entry_date", "category"]].head(10), use_container_width=True, hide_index=True)
    else:
        st.info("ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("### í™˜ê²½/ì„¤ì •")
    st.write({
        "engine": st.session_state.get("engine", DEFAULT_ENGINE),
        "has_api_key": bool(st.session_state.get("api_key")),
        "model": st.session_state.get("model", DEFAULT_MODEL),
        "db_path": DB_PATH,
        "note_policy": "ê¸°ë³¸: top ìŠ¤í‚¬ë§Œ 2+2 ë©”ëª¨. í† ê¸€ë¡œ ë‹¤ë¥¸ ìŠ¤í‚¬ í™•ì¥.",
    })

    # notes count
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM skill_notes")
        note_count = cur.fetchone()[0]
    st.write("ì €ì¥ëœ ë©”ëª¨ ê°œìˆ˜(skill_notes):", note_count)

    st.info(
        "MetaTone ë¶„ì„ì€ ìš”ì•½/STAR/íŒ¨í„´ìš”ì•½ ì—†ì´, í–‰ë™Â·ë°°ì›€ + ìŠ¤í‚¬ ê·¼ê±°/ê°œë… + (top ìŠ¤í‚¬ ê¸°ì¤€) 2+2 ë©”ëª¨ ë£¨í‹´ì— ì§‘ì¤‘í•©ë‹ˆë‹¤."
    )


if __name__ == "__main__":
    main()
