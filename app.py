import os
import re
import json
import uuid
import time
import sqlite3
from datetime import datetime, date
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ----------------------------
# OpenAI client (requires openai>=1.0.0)
# ----------------------------
try:
    from openai import OpenAI
except Exception:
    OpenAI = None


# ============================
# App Config
# ============================
APP_TITLE = "MetaTone â€” ì„±ì¥ ê¸°ë¡ ê¸°ë°˜ ì†Œí”„íŠ¸ìŠ¤í‚¬ íŠ¸ë˜ì»¤ (MVP)"
DB_PATH = "metatone.db"

DEFAULT_MODEL = "gpt-4o-mini"
MODEL_OPTIONS = [DEFAULT_MODEL, "gpt-4.1-mini", "gpt-4o"]

SOFT_SKILLS = ["ë¬¸ì œí•´ê²°", "ì˜ì‚¬ì†Œí†µ", "í˜‘ì—…", "ë¦¬ë”ì‹­", "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±", "í•™ìŠµì—­ëŸ‰"]

CATEGORIES = ["í•™ìŠµ(ìˆ˜ì—…/ìê²©ì¦/ë…ì„œ)", "í”„ë¡œì íŠ¸", "ë¦¬ë”ì‹­Â·ë™ì•„ë¦¬", "ëŒ€ì™¸í™œë™", "ê´€ê³„Â·í˜‘ì—…", "ìƒí™œÂ·ë£¨í‹´"]

ANALYSIS_ENGINES = ["ë¬´ë£Œ(ë¡œì»¬) â€” ê·œì¹™/TF-IDF", "LLM(OpenAI)"]
DEFAULT_ENGINE = ANALYSIS_ENGINES[0]


# ============================
# DB Utilities
# (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í˜¸í™˜: tags/title ì»¬ëŸ¼ì€ ë‚¨ê²¨ë‘ë˜ MetaToneì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# ============================
def get_conn() -> sqlite3.Connection:
    return sqlite3.connect(DB_PATH, check_same_thread=False, timeout=10)


def init_db() -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")
        cur.execute("""
        CREATE TABLE IF NOT EXISTS entries (
            id TEXT PRIMARY KEY,
            created_at TEXT NOT NULL,
            entry_date TEXT NOT NULL,
            category TEXT,
            tags TEXT,
            title TEXT,
            raw_text TEXT NOT NULL,
            artifacts TEXT,
            analysis_json TEXT
        )
        """)
        conn.commit()


def insert_entry(entry: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO entries (id, created_at, entry_date, category, tags, title, raw_text, artifacts, analysis_json)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            entry["id"],
            entry["created_at"],
            entry["entry_date"],
            entry.get("category"),
            json.dumps(entry.get("tags", []), ensure_ascii=False),  # MetaTone ë¯¸ì‚¬ìš©(ë¹ˆ ë¦¬ìŠ¤íŠ¸)
            entry.get("title"),  # MetaTone ë¯¸ì‚¬ìš©(None)
            entry["raw_text"],
            json.dumps(entry.get("artifacts", []), ensure_ascii=False),
            json.dumps(entry.get("analysis", {}), ensure_ascii=False)
        ))
        conn.commit()


def update_entry_analysis(entry_id: str, analysis: Dict[str, Any]) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            UPDATE entries SET analysis_json = ? WHERE id = ?
        """, (json.dumps(analysis, ensure_ascii=False), entry_id))
        conn.commit()


def fetch_entries(limit: int = 500) -> pd.DataFrame:
    with get_conn() as conn:
        df = pd.read_sql_query("""
            SELECT * FROM entries ORDER BY entry_date DESC, created_at DESC LIMIT ?
        """, conn, params=(limit,))

    def safe_json(x, default):
        if not x:
            return default
        try:
            return json.loads(x)
        except Exception:
            return default

    df["tags_parsed"] = df["tags"].apply(lambda x: safe_json(x, default=[]))
    df["artifacts_parsed"] = df["artifacts"].apply(lambda x: safe_json(x, default=[]))
    df["analysis_parsed"] = df["analysis_json"].apply(lambda x: safe_json(x, default={}))
    return df


def fetch_entry_by_id(entry_id: str) -> Optional[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("SELECT * FROM entries WHERE id = ?", (entry_id,))
        row = cur.fetchone()

    if not row:
        return None

    cols = ["id", "created_at", "entry_date", "category", "tags", "title", "raw_text", "artifacts", "analysis_json"]
    d = dict(zip(cols, row))

    for k, default in [("tags", []), ("artifacts", []), ("analysis_json", {})]:
        try:
            d[k] = json.loads(d[k]) if d[k] else default
        except Exception:
            d[k] = default
    return d


def delete_entry(entry_id: str) -> None:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("DELETE FROM entries WHERE id = ?", (entry_id,))
        conn.commit()


# ============================
# Text Similarity (local) + caching
# ============================
@st.cache_resource(show_spinner=False)
def build_similarity_index_cached(corpus: Tuple[str, ...]) -> Tuple[TfidfVectorizer, Any]:
    vectorizer = TfidfVectorizer(stop_words=None, max_features=5000)
    X = vectorizer.fit_transform(list(corpus))
    return vectorizer, X


def get_similar_entries(df: pd.DataFrame, target_text: str, top_k: int = 5) -> List[Tuple[str, float]]:
    if top_k <= 0 or df.empty:
        return []
    corpus_list = df["raw_text"].fillna("").tolist()
    if len(corpus_list) < 2:
        return []

    corpus = tuple(corpus_list)
    vectorizer, X = build_similarity_index_cached(corpus)
    try:
        x_target = vectorizer.transform([target_text])
        sims = cosine_similarity(x_target, X).flatten()
    except Exception:
        return []

    pairs = list(zip(df["id"].tolist(), sims.tolist()))
    pairs.sort(key=lambda x: x[1], reverse=True)
    return pairs[:top_k]


# ============================
# OpenAI helpers
# ============================
def get_openai_client(api_key: str):
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ë²„ì „ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. `pip install -U openai` í•´ì£¼ì„¸ìš”.")
    if not api_key or not api_key.strip():
        raise RuntimeError("OpenAI API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return OpenAI(api_key=api_key)


def strip_code_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^\s*```(?:json)?\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\s*```\s*$", "", s)
    return s.strip()


def _extract_first_json_object(s: str) -> str:
    s = strip_code_fences(s)
    start = s.find("{")
    if start == -1:
        return s

    depth = 0
    for i in range(start, len(s)):
        if s[i] == "{":
            depth += 1
        elif s[i] == "}":
            depth -= 1
            if depth == 0:
                return s[start:i + 1]
    return s


def _json_repair_minimal(s: str) -> str:
    s = s.strip()
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = re.sub(r",\s*([}\]])", r"\1", s)
    s = re.sub(r"\bTrue\b", "true", s)
    s = re.sub(r"\bFalse\b", "false", s)
    s = re.sub(r"\bNone\b", "null", s)
    return s


def robust_json_loads(s: str) -> Dict[str, Any]:
    raw = _extract_first_json_object(s)
    try:
        out = json.loads(raw)
    except Exception:
        out = json.loads(_json_repair_minimal(raw))
    if not isinstance(out, dict):
        raise ValueError("JSON ìµœìƒìœ„ê°€ ê°ì²´(dict)ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    return out


# ============================
# MetaTone: ë¶„ì„ ìŠ¤í‚¤ë§ˆ(íŒ¨í„´ ìš”ì•½ ì—†ìŒ)
# ============================
SKILL_CONCEPTS = {
    "ë¬¸ì œí•´ê²°": "ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ì›ì¸ì„ íŒŒì•…í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ ëŒ€ì•ˆì„ ë§Œë“¤ê³  ê²€ì¦í•˜ëŠ” ì—­ëŸ‰",
    "ì˜ì‚¬ì†Œí†µ": "ìƒëŒ€ì˜ ì´í•´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì¡°í™”Â·ì „ë‹¬í•˜ê³  í•©ì˜ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì—­ëŸ‰",
    "í˜‘ì—…": "ì—­í• Â·ì˜ì¡´ì„±ì„ ë§ì¶”ê³  ìƒí˜¸ ì‹ ë¢°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„±ê³¼ë¥¼ í•¨ê»˜ ë§Œë“œëŠ” ì—­ëŸ‰",
    "ë¦¬ë”ì‹­": "ë°©í–¥ì„ ì œì‹œí•˜ê³  ì˜ì‚¬ê²°ì •ì„ ë•ê³  êµ¬ì„±ì›ì´ ì›€ì§ì´ê²Œ ë§Œë“œëŠ” ì˜í–¥ë ¥",
    "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": "ì—ë„ˆì§€Â·ê°ì •Â·ì‹œê°„ì„ ê´€ë¦¬í•˜ë©° ì••ë°• ì†ì—ì„œë„ íšŒë³µí•˜ê³  ì§€ì†í•˜ëŠ” ì—­ëŸ‰",
    "í•™ìŠµì—­ëŸ‰": "í•™ìŠµ ëª©í‘œë¥¼ ì„¸ìš°ê³  í”¼ë“œë°±ì„ í†µí•´ ì§€ì‹ì„ ë‚´ ê²ƒìœ¼ë¡œ ë§Œë“œëŠ” ì—­ëŸ‰",
}


def analyze_entry_with_openai(
    api_key: str,
    model: str,
    entry: Dict[str, Any],
    related_summaries: List[Dict[str, Any]],
    output_mode: str = "portfolio"
) -> Dict[str, Any]:
    """
    output_mode:
      - "analysis_only": ìƒí™©ë¶„ì„ + ìŠ¤í‚¬ + ì„±ì¥ê³„íš + ê°œë…ì„¤ëª…
      - "portfolio": ìœ„ + STAR/ë©´ì ‘ìŠ¤í¬ë¦½íŠ¸(ì„ íƒ ìœ ì§€)
    """
    client = get_openai_client(api_key)

    persona = (
        "ë‹¹ì‹ ì€ 'MetaTone'ì˜ ì½”ì¹˜ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒí™©ì„ ìš”ì•½Â·ë¶„ì„í•˜ê³ , ê·¸ ìƒí™©ì—ì„œ ìŒ“ì¸ ì†Œí”„íŠ¸ìŠ¤í‚¬ì„ ì¦ê±°(ì›ë¬¸ ë°œì·Œ)ë¡œ ì—°ê²°í•©ë‹ˆë‹¤. "
        "ê³¼ì¥Â·ë¯¸ì‚¬ì—¬êµ¬ ê¸ˆì§€, ë‹¨ì • ê¸ˆì§€, ê·¼ê±° ì¤‘ì‹¬."
    )

    # related summaries: keep short
    related_block = []
    for rs in (related_summaries or [])[:5]:
        related_block.append({
            "id": rs.get("id"),
            "date": rs.get("entry_date"),
            "one_liner": rs.get("one_liner", ""),
            "skills": rs.get("skills", []),
        })

    want_portfolio = (output_mode == "portfolio")

    output_contract: Dict[str, Any] = {
        "meta": {
            "entry_id": entry["id"],
            "entry_date": entry["entry_date"],
            "category": entry.get("category") or ""
        },
        "situation_analysis": {
            "summary": "2~3ë¬¸ì¥ ìƒí™© ìš”ì•½",
            "challenge": "í•µì‹¬ ë‚œì /ì œì•½ 1~2ê°œ",
            "your_actions": "ë³¸ì¸ì´ ì‹¤ì œë¡œ í•œ í–‰ë™(êµ¬ì²´) 2~4ê°œ",
            "outcome": "ê²°ê³¼/ë³€í™”(ê°€ëŠ¥í•˜ë©´ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„)",
            "learning": "ë°°ìš´ ì  1~2ë¬¸ì¥"
        },
        "soft_skills": [
            {
                "name": "í˜‘ì—…",
                "confidence": 0.0,
                "evidence_quotes": ["ì›ë¬¸ ê·¸ëŒ€ë¡œ ì§§ê²Œ 1~2ê°œ(ê° 80ì ì´ë‚´)"],
                "why_it_counts": "ì™œ ì´ ì—­ëŸ‰ì¸ì§€ í•œ ë¬¸ì¥",
                "concept": "ì´ ì—­ëŸ‰ì˜ ê°œë… ì„¤ëª…(1ë¬¸ì¥)"
            }
        ],
        "growth_plan": {
            "what_to_develop_next": ["ë‹¤ìŒì— ë°œì „ì‹œí‚¤ë©´ ì¢‹ì€ ì—­ëŸ‰ 1~2ê°œ(ì†Œí”„íŠ¸ìŠ¤í‚¬ ì´ë¦„)"],
            "how_to_practice": ["ë‚´ì¼/ë‹¤ìŒì£¼ì— í•  ìˆ˜ ìˆëŠ” ì—°ìŠµ/ë£¨í‹´ 2~4ê°œ(í–‰ë™í˜•)"],
            "reflection_questions": ["ë‹¤ìŒ ê¸°ë¡ì— í¬í•¨í•˜ë©´ ì¢‹ì€ ì§ˆë¬¸ 2~3ê°œ"]
        }
    }

    if want_portfolio:
        output_contract["portfolio"] = {
            "star_paragraph": "4~6ë¬¸ì¥",
            "interview_script_1min": "",
            "keywords": []
        }

    user_payload = {
        "entry": {
            "entry_date": entry["entry_date"],
            "category": entry.get("category"),
            "raw_text": entry["raw_text"],
            "artifacts": entry.get("artifacts") or []
        },
        "related_entries_hint": related_block,
        "soft_skill_candidates": SOFT_SKILLS,
        "skill_concepts": SKILL_CONCEPTS,
        "output_contract_example": output_contract
    }

    instructions = (
        "ê·œì¹™:\n"
        "1) ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥ (ë§ˆí¬ë‹¤ìš´/ì½”ë“œíœìŠ¤/ì„¤ëª…ë¬¸ ê¸ˆì§€)\n"
        "2) soft_skillsëŠ” 1~3ê°œë§Œ ì„ íƒ, confidenceëŠ” 0~1 ìˆ«ì\n"
        "3) evidence_quotesëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ, ìµœëŒ€ 2ê°œ, ê° 80ì ì´ë‚´\n"
        "4) conceptëŠ” ì œê³µëœ skill_conceptsë¥¼ ì°¸ê³ í•˜ë˜, ë¬¸ì¥ 1ê°œë¡œ ê°„ë‹¨íˆ\n"
        "5) ì„±ì¥ê³„íšì€ 'êµ¬ì²´ì  í–‰ë™' ìœ„ì£¼ë¡œ\n"
        "6) ê³¼ì¥/ë¯¸ì‚¬ì—¬êµ¬/ë‹¨ì • ê¸ˆì§€\n"
    )

    resp = client.chat.completions.create(
        model=model,
        temperature=0.4,
        messages=[
            {"role": "system", "content": persona},
            {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
            {"role": "user", "content": instructions},
        ]
    )
    return robust_json_loads(resp.choices[0].message.content or "")


# ============================
# ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„: MetaTone í¬ë§·
# ============================
def analyze_entry_local(
    entry: Dict[str, Any],
    related_summaries: List[Dict[str, Any]],
    output_mode: str = "portfolio"
) -> Dict[str, Any]:
    text = (entry.get("raw_text") or "").strip()

    lines = [l.strip() for l in re.split(r"[\n\r]+", text) if l.strip()]
    first = lines[0] if lines else ""
    second = lines[1] if len(lines) >= 2 else ""
    last = lines[-1] if lines else ""

    # ìƒí™© ìš”ì•½(ë³´ìˆ˜ì ìœ¼ë¡œ)
    summary = first[:160] if first else "ê¸°ë¡ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì“°ë©´ ìƒí™© ìš”ì•½ì´ ì„ ëª…í•´ì§‘ë‹ˆë‹¤."
    challenge = ""
    for kw in ["ì–´ë ¤", "ë¬¸ì œ", "ê°ˆë“±", "ì••ë°•", "ì‹¤ìˆ˜", "ë¦¬ìŠ¤í¬", "ë§‰í˜”", "í˜ë“¤"]:
        if kw in text:
            challenge = "ê¸°ë¡ì—ì„œ ë‚œì /ì œì•½(ë¬¸ì œÂ·ì••ë°•Â·ê°ˆë“± ë“±)ì´ ë“œëŸ¬ë‚©ë‹ˆë‹¤."
            break
    if not challenge:
        challenge = "ë‚œì /ì œì•½ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ë¶™ì´ë©´ ë¶„ì„ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤."

    your_actions = []
    for kw in ["í–ˆë‹¤", "ì§„í–‰", "ì •ë¦¬", "ê³µìœ ", "ì„¤ëª…", "ì¡°ìœ¨", "í™•ì¸", "ê°œì„ ", "ì‹œë„", "ê²°ì •", "ë¶„ì„"]:
        for l in lines:
            if kw in l and l not in your_actions:
                your_actions.append(l[:120])
            if len(your_actions) >= 4:
                break
        if len(your_actions) >= 4:
            break
    if not your_actions:
        your_actions = ["ë‚´ê°€ ì‹¤ì œë¡œ í•œ í–‰ë™(ì˜ˆ: ì¡°ìœ¨/ì •ë¦¬/ë¶„ì„/ê³µìœ )ì„ 2~3ê°œ ë¬¸ì¥ìœ¼ë¡œ ì ì–´ë³´ì„¸ìš”."]

    outcome = ""
    for kw in ["ê²°ê³¼", "ì™„ë£Œ", "ì„±ê³µ", "ê°œì„ ", "ë³€í™”", "ë‹¬ì„±", "ì¤„ì—ˆ", "ëŠ˜ì—ˆ", "ì¢‹ì•„ì¡Œ"]:
        if kw in text:
            outcome = "ê¸°ë¡ì—ì„œ ê²°ê³¼/ë³€í™”ê°€ ì–¸ê¸‰ë©ë‹ˆë‹¤. (ê°€ëŠ¥í•˜ë©´ ìˆ˜ì¹˜Â·ê´€ì°°ë¡œ ë³´ê°• ì¶”ì²œ)"
            break
    if not outcome:
        outcome = second[:160] if second else "ê²°ê³¼(ë¬´ì—‡ì´ ë‹¬ë¼ì¡ŒëŠ”ì§€)ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ê°€í•´ë³´ì„¸ìš”."

    learning = ""
    for kw in ["ë°°ì› ", "ê¹¨ë‹¬", "ë‹¤ìŒ", "ê°œì„ ", "ë°˜ì„±", "ëŠê¼ˆ", "ì•Œê²Œ", "êµí›ˆ", "ì„±ì°°"]:
        if kw in text:
            learning = last[:160]
            break
    if not learning:
        learning = "ë°°ìš´ ì (ë‹¤ìŒì— ì ìš©í•  ê¸°ì¤€/ì›ì¹™)ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê¸°ë©´ ëˆ„ì  íŠ¸ë˜í‚¹ì´ ì‰¬ì›Œì ¸ìš”."

    # ìŠ¤í‚¬ ë£°
    skill_rules = {
        "ë¬¸ì œí•´ê²°": ["ë¬¸ì œ", "ì›ì¸", "í•´ê²°", "ë¶„ì„", "ë””ë²„ê¹…", "êµ¬ì¡°", "ëŒ€ì•ˆ", "ê°œì„ "],
        "ì˜ì‚¬ì†Œí†µ": ["ì„¤ëª…", "ê³µìœ ", "ë°œí‘œ", "ì„¤ë“", "ì •ë¦¬", "ë¬¸ì„œ", "í”¼ë“œë°±", "í•©ì˜"],
        "í˜‘ì—…": ["íŒ€", "í˜‘ì—…", "ì¡°ìœ¨", "ì—­í• ", "íšŒì˜", "ê°ˆë“±", "ë™ë£Œ", "í•¨ê»˜"],
        "ë¦¬ë”ì‹­": ["ì£¼ë„", "ë¦¬ë“œ", "ê²°ì •", "ë°©í–¥", "ê°€ì´ë“œ", "ì½”ì¹­", "ì±…ì„", "ê¸°íš"],
        "ìê¸°ê´€ë¦¬/íšŒë³µíƒ„ë ¥ì„±": ["ì‹œê°„", "ë£¨í‹´", "íšŒë³µ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì••ë°•", "ìš°ì„ ìˆœìœ„", "ì§€ì†", "ì»¨ë””ì…˜"],
        "í•™ìŠµì—­ëŸ‰": ["ê³µë¶€", "í•™ìŠµ", "ì •ë¦¬", "ë³µìŠµ", "ì‹¤í—˜", "ê°œë…", "ê°•ì˜", "ë…ì„œ", "ì—°ìŠµ"],
    }
    scores = {k: 0 for k in SOFT_SKILLS}
    for sk, kws in skill_rules.items():
        for kw in kws:
            if kw in text:
                scores[sk] += 1

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    picked = [(k, v) for k, v in ranked if v > 0][:3]
    if not picked:
        picked = [("í•™ìŠµì—­ëŸ‰", 1)]

    # ê·¼ê±° ë¬¸ì¥(ë³´ìˆ˜ì ìœ¼ë¡œ)
    sentences = re.split(r"[.!?\n]", text)
    sentences = [s.strip() for s in sentences if s.strip()]

    evidence = {k: [] for k in SOFT_SKILLS}
    for sk, _v in picked:
        kws = skill_rules.get(sk, [])
        for s in sentences:
            if any(kw in s for kw in kws):
                evidence[sk].append(s[:80])
                if len(evidence[sk]) >= 2:
                    break

    max_score = max(v for _, v in picked) if picked else 1
    soft_skills = []
    for sk, v in picked:
        conf = 0.4 + 0.6 * (v / max_score) if max_score > 0 else 0.5
        soft_skills.append({
            "name": sk,
            "confidence": round(min(max(conf, 0.0), 1.0), 2),
            "evidence_quotes": evidence[sk][:2] if evidence[sk] else (sentences[:1] if sentences else []),
            "why_it_counts": "ì›ë¬¸ì—ì„œ í•´ë‹¹ í–‰ë™ ë‹¨ì„œ(í‚¤ì›Œë“œ/í–‰ë™ í‘œí˜„)ê°€ ë³´ì—¬ì„œ ì´ ì—­ëŸ‰ì„ ìŒ“ì€ ê²ƒìœ¼ë¡œ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. (ë¬´ë£Œ ë¡œì»¬ ë¶„ì„)",
            "concept": SKILL_CONCEPTS.get(sk, "")
        })

    # ì„±ì¥ í”Œëœ(ë‹¤ìŒ ì—­ëŸ‰): í˜„ì¬ ì„ íƒëœ ê²ƒ ì¤‘ confidence ë‚®ì€ ê²ƒ ë³´ì™„ + ì¸ì ‘ ìŠ¤í‚¬ ì¶”ì²œ(ë‹¨ìˆœ)
    next_candidates = [s["name"] for s in soft_skills[1:3]] or [soft_skills[0]["name"]]
    next_candidates = list(dict.fromkeys(next_candidates))[:2]

    how_to_practice = [
        "ê¸°ë¡ì— 'ë‚´ê°€ ì„ íƒí•œ ê¸°ì¤€(ìš°ì„ ìˆœìœ„/ê·¼ê±°/í•©ì˜ ë°©ì‹)'ì„ 1ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê¸°ê¸°",
        "ê²°ê³¼ë¥¼ ê´€ì°° ê°€ëŠ¥í•œ í‘œí˜„(ì „/í›„ ë³€í™”, ì‹œê°„/íšŸìˆ˜/í’ˆì§ˆ)ë¡œ ì ê¸°",
        "ìƒëŒ€ì™€ ìƒí˜¸ì‘ìš©ì´ ìˆì—ˆë‹¤ë©´ 'ë‚´ê°€ í•œ ë§/ìš”ì²­/ì •ë¦¬'ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‚¨ê¸°ê¸°",
    ]
    reflection_questions = [
        "ë‚´ê°€ í•œ ì„ íƒì˜ ê¸°ì¤€ì€ ë¬´ì—‡ì´ì—ˆë‚˜?",
        "ë‹¤ìŒì— ê°™ì€ ìƒí™©ì´ë©´ ë¬´ì—‡ì„ ìœ ì§€/ë³€ê²½í• ê¹Œ?",
        "ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë‚˜ ê´€ì°°ë¡œ í‘œí˜„í•œë‹¤ë©´ ë¬´ì—‡ì´ ë ê¹Œ?",
    ]

    out: Dict[str, Any] = {
        "meta": {
            "entry_id": entry["id"],
            "entry_date": entry["entry_date"],
            "category": entry.get("category") or ""
        },
        "situation_analysis": {
            "summary": summary,
            "challenge": challenge,
            "your_actions": your_actions,
            "outcome": outcome,
            "learning": learning
        },
        "soft_skills": soft_skills,
        "growth_plan": {
            "what_to_develop_next": next_candidates,
            "how_to_practice": how_to_practice,
            "reflection_questions": reflection_questions
        }
    }

    if output_mode == "portfolio":
        # ì˜µì…˜ ìœ ì§€(ì›í•˜ë©´ UIì—ì„œ ìˆ¨ê²¨ë„ ë¨). íŒ¨í„´ìš”ì•½ì€ ì—†ìŒ.
        star_parts = [
            f"ìƒí™©: {summary}",
            f"ë‚œì : {challenge}",
            f"í–‰ë™: {', '.join(your_actions[:3])}" if your_actions else "í–‰ë™: (ê¸°ë¡ ë³´ê°• í•„ìš”)",
            f"ê²°ê³¼: {outcome}",
            f"ë°°ì›€: {learning}",
        ]
        out["portfolio"] = {
            "star_paragraph": " ".join([p for p in star_parts if p]),
            "interview_script_1min": " ".join([star_parts[0], star_parts[2], star_parts[3], star_parts[4]]),
            "keywords": [s["name"] for s in soft_skills]
        }

    return out


# ============================
# ëˆ„ì  ìŠ¤í‚¬ ê³„ì‚°/í‘œí˜„
# ============================
def compute_skill_totals(df: pd.DataFrame) -> Dict[str, int]:
    totals = {s: 0 for s in SOFT_SKILLS}
    if df.empty:
        return totals
    for an in df["analysis_parsed"].tolist():
        if not isinstance(an, dict):
            continue
        skills = an.get("soft_skills") or []
        if not isinstance(skills, list):
            continue
        for sk in skills:
            if isinstance(sk, dict):
                name = sk.get("name")
                if name in totals:
                    totals[name] += 1
    return totals


def render_skill_totals(totals: Dict[str, int]) -> None:
    st.subheader("ğŸ“ˆ ì†Œí”„íŠ¸ìŠ¤í‚¬ ëˆ„ì (ë²”ì£¼ë³„)")
    cols = st.columns(3)
    items = list(totals.items())
    for i, (k, v) in enumerate(items):
        with cols[i % 3]:
            st.metric(label=k, value=v)

    # í‘œë¡œë„ ì œê³µ
    df_tot = pd.DataFrame([{"soft_skill": k, "count": v} for k, v in totals.items()]).sort_values("count", ascending=False)
    st.dataframe(df_tot, use_container_width=True, hide_index=True)


# ============================
# UI Helpers (MetaTone ì „ìš© ì¶œë ¥)
# ============================
def format_analysis_block(analysis: Dict[str, Any]) -> None:
    if not analysis or not isinstance(analysis, dict):
        st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader("ğŸ§  ìƒí™© ë¶„ì„")
    s = analysis.get("situation_analysis", {}) or {}
    st.markdown("**ìš”ì•½**")
    st.write(s.get("summary", ""))
    st.markdown("**ë‚œì /ì œì•½**")
    st.write(s.get("challenge", ""))
    st.markdown("**ë‚´ í–‰ë™**")
    actions = s.get("your_actions") or []
    if isinstance(actions, list):
        for a in actions:
            st.write(f"- {a}")
    else:
        st.write(actions)
    st.markdown("**ê²°ê³¼/ë³€í™”**")
    st.write(s.get("outcome", ""))
    st.markdown("**ë°°ì›€**")
    st.write(s.get("learning", ""))

    st.subheader("ğŸ¯ ì˜¤ëŠ˜ ìŒ“ì€ ì†Œí”„íŠ¸ ìŠ¤í‚¬")
    skills = analysis.get("soft_skills", []) or []
    if not skills:
        st.write("ì„ ì •ëœ ì—­ëŸ‰ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for sk in skills:
            if not isinstance(sk, dict):
                continue
            name = sk.get("name", "")
            conf = sk.get("confidence", 0)
            try:
                conf = float(conf)
            except Exception:
                conf = 0.0

            st.markdown(f"- **{name}** (confidence: {conf:.2f})")

            ev = sk.get("evidence_quotes", []) or []
            if isinstance(ev, list) and ev:
                for q in ev[:2]:
                    st.caption(f"ê·¼ê±°: â€œ{q}â€")

            st.write(sk.get("why_it_counts", ""))

            concept = sk.get("concept") or SKILL_CONCEPTS.get(name, "")
            if concept:
                st.info(f"ê°œë…: {concept}")

    st.subheader("ğŸš€ ì•ìœ¼ë¡œ ë°œì „ì‹œí‚¤ë©´ ì¢‹ì€ ì—­ëŸ‰")
    gp = analysis.get("growth_plan", {}) or {}
    nxt = gp.get("what_to_develop_next") or []
    if isinstance(nxt, list) and nxt:
        st.write("ë‹¤ìŒ ì—­ëŸ‰(ì¶”ì²œ): " + ", ".join(nxt))
    practice = gp.get("how_to_practice") or []
    if practice:
        st.markdown("**ì—°ìŠµ/ë£¨í‹´ ì œì•ˆ**")
        for p in practice[:6]:
            st.write(f"- {p}")
    qs = gp.get("reflection_questions") or []
    if qs:
        st.markdown("**ë‹¤ìŒ ê¸°ë¡ì— ë„ì›€ì´ ë˜ëŠ” ì§ˆë¬¸**")
        for q in qs[:6]:
            st.write(f"- {q}")

    port = analysis.get("portfolio")
    if isinstance(port, dict) and port.get("star_paragraph"):
        st.subheader("ğŸ“ (ì˜µì…˜) STAR/ë©´ì ‘ ìŠ¤í¬ë¦½íŠ¸")
        st.markdown("**STAR ë¬¸ë‹¨**")
        st.write(port.get("star_paragraph", ""))
        st.markdown("**ë©´ì ‘ 1ë¶„ ìŠ¤í¬ë¦½íŠ¸**")
        st.write(port.get("interview_script_1min", ""))
        st.markdown("**í‚¤ì›Œë“œ**")
        st.write(", ".join(port.get("keywords", []) or []))


def summarize_for_related(df: pd.DataFrame) -> List[Dict[str, Any]]:
    summaries: List[Dict[str, Any]] = []
    for _, r in df.iterrows():
        an = r.get("analysis_parsed") or {}
        one_liner = ""
        skills: List[str] = []
        try:
            s = (an.get("situation_analysis", {}) or {}) if isinstance(an, dict) else {}
            one_liner = (s.get("learning") or s.get("outcome") or s.get("summary") or "")[:80]
            soft = (an.get("soft_skills") or []) if isinstance(an, dict) else []
            skills = [x.get("name") for x in soft if isinstance(x, dict) and x.get("name")]
        except Exception:
            pass
        summaries.append({
            "id": r["id"],
            "entry_date": r["entry_date"],
            "one_liner": one_liner,
            "skills": skills
        })
    return summaries


# ============================
# Engine switch wrapper (LLM ì‹¤íŒ¨ ì‹œ ë¬´ë£Œ fallback)
# ============================
def run_analysis_engine(
    engine: str,
    entry: Dict[str, Any],
    related: List[Dict[str, Any]],
    output_mode: str
) -> Dict[str, Any]:
    if engine.startswith("ë¬´ë£Œ"):
        return analyze_entry_local(entry=entry, related_summaries=related, output_mode=output_mode)

    api_key = st.session_state.get("api_key", "")
    if not api_key:
        st.warning("LLM ë¶„ì„ì„ ì„ íƒí–ˆì§€ë§Œ API Keyê°€ ì—†ì–´ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return analyze_entry_local(entry=entry, related_summaries=related, output_mode=output_mode)

    try:
        return analyze_entry_with_openai(
            api_key=api_key,
            model=st.session_state.get("model", DEFAULT_MODEL),
            entry=entry,
            related_summaries=related,
            output_mode=output_mode
        )
    except Exception as e:
        st.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨ â†’ ë¬´ë£Œ(ë¡œì»¬) ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n\nì‚¬ìœ : {e}")
        return analyze_entry_local(entry=entry, related_summaries=related, output_mode=output_mode)


# ============================
# Streamlit App
# ============================
def main():
    st.set_page_config(page_title="MetaTone", layout="wide")
    init_db()

    # Sidebar settings
    st.sidebar.title("âš™ï¸ Settings")
    api_key_env = os.getenv("OPENAI_API_KEY", "")
    api_key_input = st.sidebar.text_input(
        "OpenAI API Key (ì„ íƒ)",
        value=st.session_state.get("api_key", api_key_env),
        type="password"
    )
    st.session_state["api_key"] = (api_key_input or "").strip()

    current_model = st.session_state.get("model", DEFAULT_MODEL)
    if current_model not in MODEL_OPTIONS:
        current_model = DEFAULT_MODEL
    model_index = MODEL_OPTIONS.index(current_model)
    st.sidebar.selectbox("Model (LLM ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)", options=MODEL_OPTIONS, index=model_index, key="model")

    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ§  ë¶„ì„ ì—”ì§„")
    st.sidebar.selectbox("ë¶„ì„ ë°©ì‹", options=ANALYSIS_ENGINES, index=0, key="engine")
    st.sidebar.caption("ë¬´ë£Œ(ë¡œì»¬)ì€ OpenAI ì—†ì´ë„ ë™ì‘í•©ë‹ˆë‹¤. (ì¿¼í„°/ê²°ì œ ì´ìŠˆ ì—†ìŒ)")

    st.sidebar.markdown("---")
    page = st.sidebar.radio("í˜ì´ì§€", ["âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€", "ğŸ“š ê¸°ë¡ ëª©ë¡", "ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸"])

    df = fetch_entries()
    totals = compute_skill_totals(df)

    st.title(APP_TITLE)
    st.caption("ê¸°ë¡ ë³¸ë¬¸ì—ì„œ ìƒí™©ì„ ë¶„ì„í•˜ê³ , ìŒ“ì¸ ì†Œí”„íŠ¸ìŠ¤í‚¬ê³¼ ë‹¤ìŒ ì„±ì¥ ë°©í–¥ì„ ì •ë¦¬í•©ë‹ˆë‹¤. (ëˆ„ì  íŠ¸ë˜í‚¹ í¬í•¨)")

    # ëˆ„ì ì€ ëª¨ë“  í˜ì´ì§€ ìƒë‹¨ì— ë…¸ì¶œ(ì›í•˜ë©´ íŠ¹ì • í˜ì´ì§€ë§Œ ë…¸ì¶œë¡œ ë°”ê¿€ ìˆ˜ ìˆì–´ìš”)
    render_skill_totals(totals)
    st.markdown("---")

    if page == "âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€":
        render_new_entry(df)
    elif page == "ğŸ“š ê¸°ë¡ ëª©ë¡":
        render_history(df)
    else:
        render_debug(df)


# ============================
# Page: New Entry (ìš”êµ¬ì‚¬í•­ ë°˜ì˜)
# êµ¬ì„±: ë‚ ì§œ, ì¦ê±°/ìë£Œ ë§í¬(ì„ íƒ), ì¹´í…Œê³ ë¦¬(ì„ íƒ/ì§ì ‘ì…ë ¥), ê¸°ë¡ë³¸ë¬¸(í•„ìˆ˜)
# ë¶„ì„ ì˜µì…˜ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
# ============================
def render_new_entry(df: pd.DataFrame):
    st.subheader("âœï¸ ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€")

    col1, col2 = st.columns([1, 1])
    with col1:
        entry_date = st.date_input("ë‚ ì§œ", value=date.today())

        cat_choice = st.selectbox(
            "ì¹´í…Œê³ ë¦¬(ì„ íƒ)",
            options=["(ì„ íƒ ì•ˆ í•¨)"] + CATEGORIES + ["(ì§ì ‘ ì…ë ¥)"],
            index=0
        )
        cat_custom = ""
        if cat_choice == "(ì§ì ‘ ì…ë ¥)":
            cat_custom = st.text_input("ì¹´í…Œê³ ë¦¬ ì§ì ‘ ì…ë ¥", placeholder="ì˜ˆ: ì¸í„´/í˜„ì¥ì‹¤ìŠµ, ê°œì¸ í”„ë¡œì íŠ¸, ì·¨ë¯¸ í™œë™ ë“±")
        category = None
        if cat_choice == "(ì„ íƒ ì•ˆ í•¨)":
            category = None
        elif cat_choice == "(ì§ì ‘ ì…ë ¥)":
            category = cat_custom.strip() if cat_custom.strip() else None
        else:
            category = cat_choice

    with col2:
        artifacts = st.text_area(
            "ì¦ê±°/ìë£Œ ë§í¬(ì„ íƒ) â€” ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ",
            placeholder="ì˜ˆ: Notion ë§í¬, Google Doc, GitHub, ë°œí‘œìë£Œ URL ë“±"
        )
        artifacts_list = [x.strip() for x in (artifacts or "").splitlines() if x.strip()]

    raw_text = st.text_area(
        "ê¸°ë¡ ë³¸ë¬¸(í•„ìˆ˜)",
        height=240,
        placeholder="ì˜¤ëŠ˜ì˜ ìƒí™©/ë‚´ ì—­í• /ë‚´ê°€ í•œ í–‰ë™/ê²°ê³¼/ë°°ì›€ ì¤‘ì‹¬ìœ¼ë¡œ ì ì–´ì£¼ì„¸ìš”."
    )

    st.markdown("### ğŸ” ë¶„ì„ ì˜µì…˜")
    do_analysis = st.checkbox("ì €ì¥ í›„ ë¶„ì„ ì‹¤í–‰í•˜ê¸°", value=True)
    output_mode_label = st.selectbox("ì‚°ì¶œë¬¼ ë²”ìœ„", options=["í¬íŠ¸í´ë¦¬ì˜¤ê¹Œì§€(ì¶”ì²œ)", "ë¶„ì„ë§Œ"], index=0)
    top_k = st.slider("ìœ ì‚¬ ê¸°ë¡ ì¶”ì²œ(top-k)", min_value=0, max_value=10, value=5)

    if st.button("âœ… ì €ì¥", type="primary"):
        if not (raw_text or "").strip():
            st.error("ê¸°ë¡ ë³¸ë¬¸ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
            return

        entry_id = str(uuid.uuid4())
        created_at = datetime.now().isoformat(timespec="seconds")
        entry = {
            "id": entry_id,
            "created_at": created_at,
            "entry_date": entry_date.isoformat(),
            "category": category,
            "tags": [],            # MetaTone ë¯¸ì‚¬ìš©
            "title": None,         # MetaTone ë¯¸ì‚¬ìš©
            "raw_text": raw_text.strip(),
            "artifacts": artifacts_list,
            "analysis": {}
        }
        insert_entry(entry)
        st.success("ê¸°ë¡ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        if not do_analysis:
            return

        # ìœ ì‚¬ ê¸°ë¡ íŒíŠ¸(ì„ íƒ)
        related: List[Dict[str, Any]] = []
        if top_k > 0 and not df.empty:
            sims = get_similar_entries(df, entry["raw_text"], top_k=top_k)
            hint_rows: List[Dict[str, Any]] = []
            for rid, _score in sims:
                r = fetch_entry_by_id(rid)
                if r:
                    hint_rows.append(r)
            hint_df = pd.DataFrame(hint_rows) if hint_rows else pd.DataFrame()
            if not hint_df.empty:
                hint_df["analysis_parsed"] = hint_df["analysis_json"].apply(
                    lambda x: x if isinstance(x, dict) else (x or {})
                )
                related = summarize_for_related(hint_df)

        engine = st.session_state.get("engine", DEFAULT_ENGINE)
        output_mode = "portfolio" if output_mode_label.startswith("í¬íŠ¸í´ë¦¬ì˜¤") else "analysis_only"

        with st.spinner("ë¶„ì„ ì¤‘..."):
            analysis = run_analysis_engine(engine=engine, entry=entry, related=related, output_mode=output_mode)
            update_entry_analysis(entry_id, analysis)
            st.success("ë¶„ì„ ì™„ë£Œ! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            format_analysis_block(analysis)


# ============================
# Page: History
# ============================
def render_history(df: pd.DataFrame):
    st.subheader("ğŸ“š ê¸°ë¡ ëª©ë¡")
    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. 'ì˜¤ëŠ˜ì˜ ê¸°ë¡ ì¶”ê°€'ì—ì„œ ì‘ì„±í•´ë³´ì„¸ìš”.")
        return

    colf1, colf2, colf3 = st.columns([1, 1, 2])
    with colf1:
        cat = st.selectbox("ì¹´í…Œê³ ë¦¬ í•„í„°", options=["(ì „ì²´)"] + CATEGORIES)
    with colf2:
        skill_filter = st.selectbox("ì†Œí”„íŠ¸ìŠ¤í‚¬ í•„í„°", options=["(ì „ì²´)"] + SOFT_SKILLS)
    with colf3:
        q = st.text_input("ê²€ìƒ‰(ë³¸ë¬¸)", placeholder="ì˜ˆ: ë°œí‘œ, ì¡°ìœ¨, íšŒë³µ, ê¸°ì¤€, í”¼ë“œë°±...")

    filtered = df.copy()

    if cat != "(ì „ì²´)":
        filtered = filtered[filtered["category"] == cat]

    if (q or "").strip():
        qq = q.strip().lower()
        filtered = filtered[filtered["raw_text"].str.lower().str.contains(qq, na=False)]

    if skill_filter != "(ì „ì²´)":
        def has_skill(an):
            if not isinstance(an, dict):
                return False
            skills = an.get("soft_skills", []) or []
            return any((s.get("name") == skill_filter) for s in skills if isinstance(s, dict))
        filtered = filtered[filtered["analysis_parsed"].apply(has_skill)]

    st.caption(f"ì´ {len(filtered)}ê°œ")

    engine = st.session_state.get("engine", DEFAULT_ENGINE)

    for _, r in filtered.iterrows():
        entry_date = r.get("entry_date", "")
        category = r.get("category") or "â€”"
        an = r.get("analysis_parsed") or {}
        if not isinstance(an, dict):
            an = {}
        skills = [s.get("name") for s in (an.get("soft_skills") or []) if isinstance(s, dict) and s.get("name")]
        skill_text = ", ".join(skills) if skills else "â€”"

        with st.expander(f"{entry_date} Â· ì¹´í…Œê³ ë¦¬: {category}  |  ìŠ¤í‚¬: {skill_text}"):
            st.write(r["raw_text"])

            artifacts = r.get("artifacts_parsed") or []
            if artifacts:
                st.markdown("**ì¦ê±°/ë§í¬**")
                for a in artifacts:
                    st.write(f"- {a}")

            st.markdown("---")
            if an:
                format_analysis_block(an)
            else:
                st.info("ì•„ì§ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¶„ì„ì„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ìš”.")

            colb1, colb2, colb3 = st.columns([1, 1, 1])
            with colb1:
                if st.button("ğŸ¤– ì´ ê¸°ë¡ ë¶„ì„í•˜ê¸°", key=f"an_{r['id']}"):
                    entry = fetch_entry_by_id(r["id"])
                    if not entry:
                        st.error("ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        other = df[df["id"] != r["id"]].head(80)
                        related = summarize_for_related(other) if not other.empty else []
                        payload = {
                            "id": entry["id"],
                            "entry_date": entry["entry_date"],
                            "category": entry.get("category"),
                            "raw_text": entry["raw_text"],
                            "artifacts": entry.get("artifacts") or []
                        }
                        with st.spinner("ë¶„ì„ ì¤‘..."):
                            analysis = run_analysis_engine(engine=engine, entry=payload, related=related, output_mode="analysis_only")
                            update_entry_analysis(r["id"], analysis)
                            st.success("ë¶„ì„ ì™„ë£Œ! í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                            st.rerun()
            with colb2:
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{r['id']}"):
                    delete_entry(r["id"])
                    st.success("ì‚­ì œí–ˆìŠµë‹ˆë‹¤. í™”ë©´ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                    st.rerun()
            with colb3:
                port = (an.get("portfolio") or {}) if isinstance(an, dict) else {}
                if isinstance(port, dict) and port.get("star_paragraph"):
                    st.download_button(
                        "â¬‡ï¸ STAR ë¬¸ë‹¨ ë‹¤ìš´ë¡œë“œ(txt)",
                        data=port["star_paragraph"],
                        file_name=f"STAR_{entry_date}_{r['id'][:6]}.txt"
                    )


# ============================
# Page: Debug
# ============================
def render_debug(df: pd.DataFrame):
    st.subheader("ğŸ§ª ë””ë²„ê·¸/ë¡œê·¸")
    st.write("í˜„ì¬ DBì— ì €ì¥ëœ ê¸°ë¡ ê°œìˆ˜:", len(df))

    st.markdown("### ìµœê·¼ 10ê°œ ê¸°ë¡ ë¯¸ë¦¬ë³´ê¸°")
    if not df.empty:
        st.dataframe(df[["entry_date", "category"]].head(10), use_container_width=True, hide_index=True)
    else:
        st.info("ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("### í™˜ê²½/ì„¤ì •")
    st.write({
        "engine": st.session_state.get("engine", DEFAULT_ENGINE),
        "has_api_key": bool(st.session_state.get("api_key")),
        "model": st.session_state.get("model", DEFAULT_MODEL),
        "db_path": DB_PATH,
        "journal_mode": "WAL (init_dbì—ì„œ ì„¤ì •)",
    })

    st.info(
        "MetaToneì—ì„œëŠ” íŒ¨í„´ ìš”ì•½ì„ ì œê±°í•˜ê³ , "
        "ìƒí™© ë¶„ì„ â†’ ìŠ¤í‚¬ ë„ì¶œ â†’ ì„±ì¥ í”Œëœ â†’ ê°œë… ì„¤ëª… â†’ ëˆ„ì  íŠ¸ë˜í‚¹ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤."
    )


if __name__ == "__main__":
    main()
